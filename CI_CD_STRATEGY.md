# Supply Chain Data Engineering - CI/CD ç­–ç•¥

## é¡¹ç›®æ¶æ„æ¦‚è§ˆ
```
Kafka (Streaming) â†’ GCS raw_staging â†’ Batch Pipeline â†’ GCS transformed_data
                                            â†“
                                      BigQuery Tables
                                            â†“
                                      dbt Models (Staging + Marts)
                                            â†“
                                   Analytics Dashboard
```

---

## 1. æ•´ä½“ CI/CD æµç¨‹

### é˜¶æ®µåˆ’åˆ†
```
Push to GitHub 
    â†“
1ï¸âƒ£ Code Quality Checks (Linting, Format)
    â†“
2ï¸âƒ£ Unit Tests (Python, dbt)
    â†“
3ï¸âƒ£ Integration Tests (Streaming, Batch)
    â†“
4ï¸âƒ£ Infrastructure Validation (Terraform)
    â†“
5ï¸âƒ£ Build & Push Artifacts (Docker Images)
    â†“
6ï¸âƒ£ Deploy to Dev/Staging Environment
    â†“
7ï¸âƒ£ Production Deployment (Manual Approval)
```

---

## 2. å„ç»„ä»¶çš„ CI/CD æ£€æŸ¥æ¸…å•

### ğŸ“ **Python Code (Streaming + Batch Pipeline)**

#### æ£€æŸ¥é¡¹ç›®
```bash
# 1. Linting
pylint streaming_pipeline/kafka/*.py
pylint batch_pipeline/data_modeling/*.py

# 2. Code Formatting
black --check streaming_pipeline/ batch_pipeline/

# 3. Type Checking
mypy streaming_pipeline/kafka/ batch_pipeline/

# 4. Security Scanning
bandit -r streaming_pipeline/ batch_pipeline/

# 5. Dependency Scanning
safety check -r streaming_pipeline/kafka/requirements.txt
safety check -r batch_pipeline/data_modeling/requirements.txt
```

#### å•å…ƒæµ‹è¯•ç¤ºä¾‹
```python
# tests/test_consumer.py
import unittest
from streaming_pipeline.kafka.consumer import KafkaConsumer

class TestKafkaConsumer(unittest.TestCase):
    def test_connection(self):
        """æµ‹è¯• Kafka è¿æ¥"""
        consumer = KafkaConsumer()
        self.assertIsNotNone(consumer.client)
    
    def test_message_parsing(self):
        """æµ‹è¯•æ¶ˆæ¯è§£æ"""
        message = {"Order_Id": 1, "Order_Date": "01/01/2020 10:00"}
        parsed = consumer.parse_message(message)
        self.assertIn("Order_Id", parsed)
```

---

### ğŸ—ï¸ **dbt Models**

#### æ£€æŸ¥é¡¹ç›®
```bash
# 1. dbt Parse Check
dbt parse --profiles-dir dbt

# 2. dbt Compile
dbt compile --profiles-dir dbt

# 3. dbt Test (Data Quality)
dbt test --profiles-dir dbt

# 4. dbt Doc Generation
dbt docs generate --profiles-dir dbt

# 5. dbt Freshness Check
dbt source freshness --profiles-dir dbt
```

#### å¿…éœ€çš„ dbt æµ‹è¯•
```yaml
# dbt/models/staging/schema.yaml
models:
  - name: dim_order
    columns:
      - name: order_id
        tests:
          - not_null
          - unique
      - name: order_date
        tests:
          - not_null
      - name: order_item_total
        tests:
          - dbt_expectations.expect_column_values_to_be_of_type:
              column_type: numeric

  - name: fact_order
    tests:
      - dbt_utils.recency:
          datepart: day
          interval: 1
          field: order_date
```

---

### ğŸ¢ **Terraform Infrastructure**

#### æ£€æŸ¥é¡¹ç›®
```bash
# 1. Terraform Format Check
terraform fmt -check -recursive

# 2. Terraform Validate
terraform validate

# 3. Security Scanning
tfsec

# 4. Cost Estimation
terraform plan -out=tfplan
terraform show tfplan | grep "will be created"
```

#### Terraform éƒ¨ç½²æ­¥éª¤
```bash
# 1. Plan
terraform plan -out=tfplan -var-file="prod.tfvars"

# 2. Show Plan (æ‰‹åŠ¨å®¡æ ¸)
terraform show tfplan

# 3. Apply (éœ€è¦ approval)
terraform apply tfplan
```

---

### ğŸ³ **Docker Images**

#### æ„å»ºå’Œæ¨é€
```bash
# 1. Build Spark Image
docker build -t gcr.io/stellar-stream-485314-p0/spark-base:latest -f docker/spark/spark-base.Dockerfile .

# 2. Push to GCR
docker push gcr.io/stellar-stream-485314-p0/spark-base:latest

# 3. Build Kafka Consumer Image
docker build -t gcr.io/stellar-stream-485314-p0/kafka-consumer:latest \
  -f docker/kafka/Dockerfile .
```

---

## 3. GitHub Actions å·¥ä½œæµé…ç½®

### ğŸ“‹ Main CI Workflow
```yaml
# .github/workflows/ci.yml
name: CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  # Job 1: Code Quality
  code-quality:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install pylint black mypy bandit safety
          pip install -r streaming_pipeline/kafka/requirements.txt
          pip install -r batch_pipeline/data_modeling/requirements.txt
      
      - name: Run Pylint
        run: |
          pylint streaming_pipeline/kafka/*.py
          pylint batch_pipeline/data_modeling/*.py
        continue-on-error: true
      
      - name: Check code format
        run: black --check streaming_pipeline/ batch_pipeline/
      
      - name: Run type check
        run: mypy streaming_pipeline/kafka/ batch_pipeline/
      
      - name: Security check
        run: |
          bandit -r streaming_pipeline/ batch_pipeline/
          safety check

  # Job 2: dbt Validation
  dbt-validation:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dbt
        run: pip install dbt-bigquery
      
      - name: dbt Parse
        working-directory: dbt
        run: dbt parse --profiles-dir .
      
      - name: dbt Compile
        working-directory: dbt
        run: dbt compile --profiles-dir .

  # Job 3: Terraform Validation
  terraform-validation:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
      
      - name: Terraform Format Check
        run: terraform fmt -check -recursive terraform/
      
      - name: Terraform Validate
        run: terraform validate
        working-directory: terraform/
      
      - name: TFSec Security Scan
        run: |
          pip install tfsec
          tfsec terraform/

  # Job 4: Unit Tests
  unit-tests:
    runs-on: ubuntu-latest
    services:
      kafka:
        image: confluentinc/cp-kafka:latest
        env:
          KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install test dependencies
        run: |
          pip install pytest pytest-cov
          pip install -r streaming_pipeline/kafka/requirements.txt
          pip install -r batch_pipeline/data_modeling/requirements.txt
      
      - name: Run tests
        run: pytest tests/ --cov=streaming_pipeline --cov=batch_pipeline

  # Job 5: Coverage Report
  coverage:
    runs-on: ubuntu-latest
    needs: unit-tests
    steps:
      - uses: actions/checkout@v3
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
```

### ğŸ“‹ Deployment Workflow (Dev/Staging/Prod)
```yaml
# .github/workflows/deploy.yml
name: Deploy Pipeline

on:
  workflow_dispatch:  # Manual trigger
    inputs:
      environment:
        description: 'Environment to deploy'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - staging
          - prod
      approve:
        description: 'I have reviewed all changes'
        required: true
        type: boolean

env:
  PROJECT_ID: stellar-stream-485314-p0
  GCR_HOSTNAME: gcr.io

jobs:
  deploy-infrastructure:
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment }}
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Terraform Plan
        working-directory: terraform/
        run: |
          terraform init -backend-config="bucket=supply-chain-tf-state-${{ github.event.inputs.environment }}"
          terraform plan -out=tfplan -var-file="${{ github.event.inputs.environment }}.tfvars"
      
      - name: Terraform Apply
        working-directory: terraform/
        if: github.event.inputs.approve == 'true'
        run: terraform apply tfplan

  build-docker-images:
    runs-on: ubuntu-latest
    needs: deploy-infrastructure
    strategy:
      matrix:
        image:
          - name: spark-base
            dockerfile: docker/spark/spark-base.Dockerfile
          - name: kafka-consumer
            dockerfile: docker/kafka/Dockerfile
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
      
      - name: Configure Docker for GCR
        run: |
          gcloud auth configure-docker ${{ env.GCR_HOSTNAME }}
      
      - name: Build Docker image
        run: |
          docker build \
            -t ${{ env.GCR_HOSTNAME }}/${{ env.PROJECT_ID }}/${{ matrix.image.name }}:${{ github.sha }} \
            -t ${{ env.GCR_HOSTNAME }}/${{ env.PROJECT_ID }}/${{ matrix.image.name }}:latest \
            -f ${{ matrix.image.dockerfile }} .
      
      - name: Push to GCR
        run: |
          docker push ${{ env.GCR_HOSTNAME }}/${{ env.PROJECT_ID }}/${{ matrix.image.name }}:${{ github.sha }}
          docker push ${{ env.GCR_HOSTNAME }}/${{ env.PROJECT_ID }}/${{ matrix.image.name }}:latest

  deploy-dbt-models:
    runs-on: ubuntu-latest
    needs: deploy-infrastructure
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dbt
        run: pip install dbt-bigquery
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: dbt Deps
        working-directory: dbt
        run: dbt deps
      
      - name: dbt Seed
        working-directory: dbt
        run: dbt seed --target ${{ github.event.inputs.environment }}
      
      - name: dbt Run (Staging)
        working-directory: dbt
        run: dbt run --select staging --target ${{ github.event.inputs.environment }}
      
      - name: dbt Test
        working-directory: dbt
        run: dbt test --target ${{ github.event.inputs.environment }}
      
      - name: dbt Run (Marts)
        working-directory: dbt
        if: success()
        working-directory: dbt
        run: dbt run --select marts --target ${{ github.event.inputs.environment }}
      
      - name: dbt Docs Generate
        working-directory: dbt
        run: dbt docs generate --target ${{ github.event.inputs.environment }}
      
      - name: Upload dbt Docs
        uses: actions/upload-artifact@v3
        with:
          name: dbt-docs
          path: dbt/target/

  integration-tests:
    runs-on: ubuntu-latest
    needs: [build-docker-images, deploy-dbt-models]
    steps:
      - uses: actions/checkout@v3
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Run integration tests
        run: |
          python -m pytest tests/integration/ \
            --environment=${{ github.event.inputs.environment }} \
            --project-id=${{ env.PROJECT_ID }}

  notify-completion:
    runs-on: ubuntu-latest
    needs: [integration-tests]
    if: always()
    steps:
      - name: Send Slack notification
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'Deployment to ${{ github.event.inputs.environment }} ${{ job.status }}'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        if: always()
```

---

## 4. æœ¬åœ°å¼€å‘å·¥ä½œæµ

### å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
# 1. åˆ›å»ºå¼€å‘åˆ†æ”¯
git checkout -b feature/your-feature

# 2. å®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r streaming_pipeline/kafka/requirements.txt
pip install -r batch_pipeline/data_modeling/requirements.txt
pip install dbt-bigquery

# 3. è¿è¡Œæœ¬åœ°æ£€æŸ¥
black .
pylint streaming_pipeline/ batch_pipeline/
mypy streaming_pipeline/

# 4. è¿è¡Œå•å…ƒæµ‹è¯•
pytest tests/ --cov

# 5. è¿è¡Œ dbt éªŒè¯
cd dbt
dbt deps
dbt parse
dbt compile
dbt test

# 6. æ¨é€ä»£ç 
git push origin feature/your-feature
```

### Pre-commit Hook é…ç½®
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/psf/black
    rev: 23.1.0
    hooks:
      - id: black
        language_version: python3.11

  - repo: https://github.com/PyCQA/isort
    rev: 5.12.0
    hooks:
      - id: isort

  - repo: https://github.com/PyCQA/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
        additional_dependencies: [flake8-docstrings]

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-merge-conflict

  - repo: https://github.com/terraform-docs/terraform-docs
    rev: v0.17.0
    hooks:
      - id: terraform-docs
```

---

## 5. ç¯å¢ƒé…ç½®ç®¡ç†

### ç¯å¢ƒå˜é‡æ¨¡æ¿
```bash
# .env.example
# Kafka
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC=supply_chain_orders

# GCP
GCP_PROJECT_ID=stellar-stream-485314-p0
GCS_BUCKET=supply-chain-data-bucket-485314
BIGQUERY_DATASET=supply_chain_bigquery

# dbt
DBT_PROFILES_DIR=dbt
DBT_TARGET=dev

# Logging
LOG_LEVEL=INFO
```

### ç¯å¢ƒç‰¹å®šé…ç½®
```yaml
# dbt/profiles.yml
supply_chain_project:
  outputs:
    dev:
      type: bigquery
      project: stellar-stream-485314-p0
      dataset: supply_chain_dbt_dev
      threads: 4
      timeout_seconds: 300
      location: us-central1
      priority: interactive
    
    staging:
      type: bigquery
      project: stellar-stream-485314-p0
      dataset: supply_chain_dbt_staging
      threads: 8
      timeout_seconds: 600
      location: us-central1
      priority: interactive
    
    prod:
      type: bigquery
      project: stellar-stream-485314-p0
      dataset: supply_chain_bigquery
      threads: 16
      timeout_seconds: 900
      location: us-central1
      priority: batch
  
  target: dev
```

---

## 6. ç›‘æ§å’Œå‘Šè­¦

### å…³é”®æŒ‡æ ‡
```
Pipeline Health Checks:
â”œâ”€â”€ Streaming latency: < 5 minutes
â”œâ”€â”€ Batch success rate: > 99.9%
â”œâ”€â”€ dbt test pass rate: 100%
â”œâ”€â”€ Data freshness: < 24 hours
â”œâ”€â”€ BigQuery costs: Tracked daily
â””â”€â”€ Error rate: < 0.1%
```

### Monitoring é…ç½®ç¤ºä¾‹
```python
# monitoring/pipeline_monitor.py
from google.cloud import monitoring_v3

def create_alert_policy():
    """åˆ›å»º BigQuery æ•°æ®æ–°é²œåº¦å‘Šè­¦"""
    client = monitoring_v3.AlertPolicyServiceClient()
    
    policy = {
        "display_name": "Order Data Freshness Alert",
        "conditions": [
            {
                "display_name": "Data not refreshed in 24 hours",
                "condition_threshold": {
                    "filter": """
                        resource.type="bigquery_resource"
                        AND metric.type="bigquery.googleapis.com/job/num_in_flight_jobs"
                    """,
                    "comparison": monitoring_v3.ComparisonType.COMPARISON_LT,
                    "threshold_value": 1,
                    "duration": "3600s"
                }
            }
        ],
        "notification_channels": [SLACK_CHANNEL_ID],
    }
    
    return client.create_alert_policy(name=parent, alert_policy=policy)
```

---

## 7. éƒ¨ç½²æ¸…å•

### Pre-Deployment Checklist
- [ ] æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] dbt æµ‹è¯• 100% é€šè¿‡
- [ ] Code coverage > 80%
- [ ] æ— å®‰å…¨æ¼æ´ï¼ˆbandit, tfsecï¼‰
- [ ] Terraform plan å®¡æ ¸é€šè¿‡
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ
- [ ] æ–‡æ¡£æ›´æ–°å®Œæˆ
- [ ] å˜æ›´æ—¥å¿—æ›´æ–°

### Post-Deployment Validation
- [ ] BigQuery è¡¨æ•°æ®å®Œæ•´
- [ ] dbt æ–‡æ¡£ç”ŸæˆæˆåŠŸ
- [ ] ç›‘æ§å‘Šè­¦é…ç½®æ­£ç¡®
- [ ] æ—¥å¿—èšåˆå·¥ä½œæ­£å¸¸
- [ ] æ€§èƒ½æŒ‡æ ‡è¾¾åˆ°é¢„æœŸ
- [ ] ç›¸å…³å›¢é˜Ÿé€šçŸ¥

---

## 8. å›æ»šç­–ç•¥

### dbt å›æ»š
```bash
# æŸ¥çœ‹å†å² dbt è¿è¡Œ
dbt run-results.json

# æ¢å¤åˆ°ä¹‹å‰çš„æ¨¡å‹ç‰ˆæœ¬
git checkout main~1 -- dbt/models/
dbt run
```

### BigQuery å›æ»š
```bash
# åˆ›å»ºè¡¨å¿«ç…§ï¼ˆç”Ÿäº§ç¯å¢ƒæ¨èï¼‰
bq cp \
  stellar-stream-485314-p0:supply_chain_bigquery.fact_order \
  stellar-stream-485314-p0:supply_chain_bigquery.fact_order_backup_$(date +%s)

# ä»å¿«ç…§æ¢å¤
bq cp \
  stellar-stream-485314-p0:supply_chain_bigquery.fact_order_backup_TIMESTAMP \
  stellar-stream-485314-p0:supply_chain_bigquery.fact_order
```

### Terraform å›æ»š
```bash
terraform destroy -var-file="prod.tfvars"  # é”€æ¯èµ„æº
git revert <commit-hash>
terraform apply
```

---

## 9. æˆæœ¬ä¼˜åŒ–

### GCP æˆæœ¬ç®¡ç†
```bash
# ç›‘æ§ BigQuery æŸ¥è¯¢æˆæœ¬
bq ls --project_id=stellar-stream-485314-p0 --max_results=100 \
  -a --format=prettyjson | jq '.[] | .totalBytes'

# è®¾ç½® BigQuery slot reservation
gcloud bigquery reservations create \
  --location=us-central1 \
  --project=stellar-stream-485314-p0 \
  slot-reservation-prod

# ä½¿ç”¨åˆ†åŒºè¡¨å‡å°‘æ‰«ææ•°æ®
ALTER TABLE supply_chain_bigquery.fact_order
PARTITION BY order_date;
```

---

## 10. ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³å®æ–½ (Week 1-2)
- [ ] åˆ›å»º GitHub Actions workflow æ–‡ä»¶
- [ ] é…ç½® GCP æœåŠ¡è´¦æˆ·å’Œå¯†é’¥
- [ ] è®¾ç½®ä»£ç æ£€æŸ¥å·¥å…·
- [ ] åˆ›å»ºæµ‹è¯•æ¡†æ¶

### çŸ­æœŸè®¡åˆ’ (Week 3-4)
- [ ] éƒ¨ç½² dbt æµ‹è¯•è¦†ç›–ç‡
- [ ] è®¾ç½®ç›‘æ§å’Œå‘Šè­¦
- [ ] åˆ›å»ºéƒ¨ç½²ç®¡é“
- [ ] æ–‡æ¡£å®Œå–„

### é•¿æœŸè®¡åˆ’ (Month 2-3)
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶
- [ ] æˆæœ¬ä¼˜åŒ–åˆ†æ
- [ ] å®¹ç¾æ¢å¤è®¡åˆ’
- [ ] è‡ªåŠ¨åŒ–å›æ»šæœºåˆ¶
