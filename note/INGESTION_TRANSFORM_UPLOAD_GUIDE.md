# é¡¹ç›®å¤ç°æŒ‡å—ï¼šIngestion â†’ Transform â†’ Upload

## ğŸ“‹ é¡¹ç›®æ•°æ®æµç¨‹ï¼ˆä¸‰é˜¶æ®µï¼‰

```
Ingestion (æ•°æ®è·å–) â†’ Transform (æ•°æ®è½¬æ¢) â†’ Upload (æ•°æ®ä¸Šä¼ )
     â†“                    â†“                    â†“
  Kafka/CSV         Spark/DBT          GCSâ†’BigQuery
```

---

## **ç¬¬ä¸€é˜¶æ®µï¼šIngestionï¼ˆæ•°æ®è·å–ï¼‰**

### æ¶‰åŠçš„ç»„ä»¶ï¼š
1. **Kafka** - æ¶ˆæ¯é˜Ÿåˆ—
2. **Streaming Producer** - æ•°æ®ç”Ÿäº§è€…ï¼ˆä» CSV è¯»å–ï¼‰
3. **Mage Consumer** - ä» Kafka æ¶ˆè´¹æ•°æ®

### éœ€è¦çš„ Setupï¼š

**1. åˆ›å»ºå¿…è¦çš„ç›®å½•å’Œé…ç½®**
```bash
cd /Users/liceline/Documents/study_material/data_engineer/project_study/supply_chain/supply_chain_de

# é…ç½® GCP å‡­è¯
mkdir -p ./docker/mage
# éœ€è¦æ”¾å…¥ google-cred.jsonï¼ˆä» GCP è·å–ï¼‰
```

**2. å®‰è£…ä¾èµ–**
```bash
pip install -r requirements.txt
# éœ€è¦çš„åŒ…ï¼špyspark, confluent_kafka, dbt
```

**3. å¯åŠ¨ Kafka**
```bash
source commands.sh
start-kafka
# ç­‰å¾… 5 ç§’è®© Kafka å®Œå…¨å¯åŠ¨
sleep 5
```

**4. å¯åŠ¨æ•°æ®ç”Ÿäº§ï¼ˆStreamingï¼‰**
```bash
stream-data  # æˆ–ï¼šdocker-compose -f ./docker/streaming/docker-compose.yaml up
```

è¿™ä¸€é˜¶æ®µä¼šï¼š
- è¯»å– `./streaming_pipeline/data/` ä¸­çš„ CSV æ–‡ä»¶
- å°†æ•°æ®å‘é€åˆ° Kafka topic `supply_chain_data`
- æ•°æ®ä¼šè¢«æš‚å­˜åœ¨å†…å­˜ä¸­ç­‰å¾…å¤„ç†

---

## **ç¬¬äºŒé˜¶æ®µï¼šTransformï¼ˆæ•°æ®è½¬æ¢ï¼‰**

### 2.1 **Spark è½¬æ¢ï¼ˆRaw â†’ Silverï¼‰**

**æ¶‰åŠçš„ç»„ä»¶ï¼š**
- Apache Spark
- GCS å­˜å‚¨æ¡¶

**æ‰§è¡Œæ­¥éª¤ï¼š**

```bash
# 1. å¯åŠ¨ Spark é›†ç¾¤
start-spark
# è¿™ä¼šæ„å»º Spark Docker é•œåƒå¹¶å¯åŠ¨ä¸»èŠ‚ç‚¹å’Œå·¥ä½œèŠ‚ç‚¹

# 2. è¿è¡Œ OLAP è½¬æ¢ç®¡é“
olap-transformation-pipeline
# æ‰§è¡Œï¼špython batch_pipeline/export_to_gcs/pipeline.py
```

**å‘ç”Ÿçš„äº‹æƒ…ï¼š**
- ä» Kafka æ¶ˆè´¹æ•°æ®ï¼ˆæˆ–ä» GCS `raw_streaming/` è¯»å–ï¼‰
- æå– 7 ä¸ªç»´åº¦è¡¨ï¼š
  - customer_dimension
  - product_dimension
  - location_dimension
  - order_dimension
  - shipping_dimension
  - department_dimension
  - metadata_dimension
- å­˜å‚¨ä¸º Parquet æ ¼å¼åˆ° GCS çš„ `transformed/` è·¯å¾„ï¼ˆSilver å±‚ï¼‰

### 2.2 **Mage æ¶ˆè´¹ Kafka â†’ GCS**

```bash
# 1. å¯åŠ¨ Mage
start-mage

# 2. Mage ä¼šè‡ªåŠ¨å°† Kafka æ•°æ®å†™å…¥ GCS raw_streaming/ ç›®å½•
# è¿™ä¸ªè¿‡ç¨‹åœ¨åå°è¿è¡Œ
```

---

## **ç¬¬ä¸‰é˜¶æ®µï¼šUploadï¼ˆæ•°æ®ä¸Šä¼ åˆ° BigQueryï¼‰**

### æ¶‰åŠçš„ç»„ä»¶ï¼š
- Mage Pipeline
- Google BigQuery
- Terraform é…ç½®çš„åŸºç¡€è®¾æ–½

**æ‰§è¡Œæ­¥éª¤ï¼š**

```bash
# 1. ç¡®ä¿ Terraform å·²é…ç½® BigQuery æ•°æ®é›†
cd ./terraform
terraform init
terraform apply

# 2. è¿”å›ä¸»ç›®å½•
cd ..

# 3. è¿è¡Œ Mage çš„ GCS â†’ BigQuery å¯¼å‡ºç®¡é“
# (é€šè¿‡ API è°ƒç”¨æˆ– Mage UI)
# å‘½ä»¤é›†ä¸­å¯èƒ½æœ‰ç›¸å…³å‡½æ•°ï¼Œæˆ–åœ¨ Mage UI ä¸­æ‰‹åŠ¨è§¦å‘
```

**å‘ç”Ÿçš„äº‹æƒ…ï¼š**
- ä» GCS `transformed/` è¯»å– Silver å±‚æ•°æ®
- å¯¼å…¥åˆ° BigQuery çš„ `terraform_bigquery` æ•°æ®é›†
- æ•°æ®å¯ç”¨äºåˆ†æå’Œ DBT è½¬æ¢

---

## **å®Œæ•´çš„ä¸€é”®å¯åŠ¨è„šæœ¬å»ºè®®**

åˆ›å»ºä¸€ä¸ªæ–°çš„å¯åŠ¨è„šæœ¬ `start-full-pipeline.sh`ï¼š

```bash
#!/bin/bash

echo "ğŸš€ å¼€å§‹ Ingestion â†’ Transform â†’ Upload æµç¨‹"

# Step 1: Ingestion
echo "ğŸ“¥ ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®è·å–ï¼ˆIngestionï¼‰"
source commands.sh
start-kafka
sleep 5
start-mage
sleep 5
stream-data &
STREAM_PID=$!
sleep 10

# Step 2: Transform
echo "ğŸ”„ ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®è½¬æ¢ï¼ˆTransformï¼‰"
start-spark
sleep 10
olap-transformation-pipeline

# Step 3: Upload
echo "ğŸ“¤ ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®ä¸Šä¼ ï¼ˆUploadï¼‰"
cd terraform
terraform apply -auto-approve
cd ..

# è¿è¡Œ Mage BigQuery å¯¼å‡ºç®¡é“
echo "â¬†ï¸  å¯¼å‡ºåˆ° BigQuery..."
# è¿™é‡Œéœ€è¦è°ƒç”¨ Mage API æˆ–åœ¨ UI ä¸­æ‰‹åŠ¨è§¦å‘

echo "âœ… æµç¨‹å®Œæˆï¼"
```

---

## **å…³é”®æ–‡ä»¶ä½ç½®**

| æ–‡ä»¶ | ç”¨é€” |
|------|------|
| streaming_pipeline/producer.py | æ•°æ®ç”Ÿäº§è€… |
| streaming_pipeline/consumer.py | Kafka æ¶ˆè´¹è€… |
| batch_pipeline/export_to_gcs/pipeline.py | Spark è½¬æ¢è„šæœ¬ |
| batch_pipeline/export_to_big_query/ | Mage BigQuery å¯¼å‡ºé…ç½® |
| terraform/main.tf | GCP åŸºç¡€è®¾æ–½é…ç½® |

---

## **å¸¸è§é—®é¢˜æ’æŸ¥**

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|--------|
| GCP å‡­è¯é”™è¯¯ | éœ€è¦åœ¨ `./docker/mage/` æ”¾å…¥ `google-cred.json` |
| Kafka è¿æ¥å¤±è´¥ | æ£€æŸ¥ Kafka æ˜¯å¦å®Œå…¨å¯åŠ¨ï¼ˆç­‰å¾… 10 ç§’ï¼‰ |
| GCS æƒé™é”™è¯¯ | ç¡®ä¿ GCP æœåŠ¡è´¦å·æœ‰è¶³å¤Ÿæƒé™ |
| Spark å†…å­˜ä¸è¶³ | è°ƒæ•´ Docker Compose ä¸­çš„å†…å­˜é™åˆ¶ |

---

## **æ¶æ„æ¦‚è§ˆ**

### Ingestion é˜¶æ®µæ•°æ®æµ
```
CSV æ–‡ä»¶ â†’ Producer â†’ Kafka Topic â†’ Mage Consumer â†’ GCS raw_streaming/
```

### Transform é˜¶æ®µæ•°æ®æµ
```
GCS raw_streaming/ â†’ Spark Pipeline â†’ ç»´åº¦è¡¨æå– â†’ GCS transformed/ (Silver)
```

### Upload é˜¶æ®µæ•°æ®æµ
```
GCS transformed/ â†’ Mage Pipeline â†’ BigQuery terraform_bigquery æ•°æ®é›†
```

---

## **Tech Stack å¯¹åº”å…³ç³»**

| é˜¶æ®µ | æŠ€æœ¯æ ˆ | ä½œç”¨ |
|------|--------|------|
| Ingestion | Kafka + Mage | å®æ—¶æ•°æ®æ‘„å…¥ |
| Transform | Spark + Python | æ•°æ®æ¸…æ´—ä¸ç»´åº¦æå– |
| Upload | Mage + Terraform | æ•°æ®å¯¼å…¥ä¸åŸºç¡€è®¾æ–½ç®¡ç† |
| Analysis | DBT + BigQuery | æ•°æ®å»ºæ¨¡ä¸åˆ†æ |
| Visualization | Metabase | å¯è§†åŒ–ä¸ä»ªè¡¨æ¿ |

---

## **æ‰§è¡ŒéªŒè¯æ£€æŸ¥æ¸…å•**

- [ ] GCP å‡­è¯æ–‡ä»¶å·²é…ç½®
- [ ] Docker å·²å®‰è£…ä¸”è¿è¡Œæ­£å¸¸
- [ ] Python ä¾èµ–å·²å®‰è£… (pyspark, confluent_kafka, dbt)
- [ ] Kafka æˆåŠŸå¯åŠ¨åœ¨ port 29092
- [ ] Streaming producer æ­£åœ¨è¿è¡Œ
- [ ] Mage å®¹å™¨è¿è¡Œå¹¶ä¸” UI å¯è®¿é—® (localhost:6789)
- [ ] Spark é›†ç¾¤å·²å¯åŠ¨
- [ ] GCS å­˜å‚¨æ¡¶å·²åˆ›å»º
- [ ] BigQuery æ•°æ®é›†å·²åˆ›å»º
- [ ] Terraform çŠ¶æ€æ–‡ä»¶å·²åˆå§‹åŒ–

---

## **æ€§èƒ½ä¼˜åŒ–å»ºè®®**

1. **Batch å¤§å°ä¼˜åŒ–** - è°ƒæ•´ Kafka batch å¤§å°ä»¥å¹³è¡¡ååé‡å’Œå»¶è¿Ÿ
2. **Spark åˆ†åŒº** - æ ¹æ®æ•°æ®é‡è°ƒæ•´ Spark åˆ†åŒºæ•°
3. **GCS å­˜å‚¨ç±»** - ä½¿ç”¨ Standard å­˜å‚¨ç”¨äºç»å¸¸è®¿é—®çš„æ•°æ®
4. **BigQuery æ§½ä½** - è€ƒè™‘ä½¿ç”¨ BigQuery æ§½ä½ä»¥è·å¾—ç¨³å®šçš„æ€§èƒ½

