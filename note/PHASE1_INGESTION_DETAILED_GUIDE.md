# ç¬¬ä¸€é˜¶æ®µï¼šä»é›¶å¼€å§‹çš„æ•°æ®è·å–ï¼ˆIngestionï¼‰è¯¦ç»†æŒ‡å—

> å‡è®¾ä½ å®Œå…¨ä¸æ‡‚ï¼Œä»é›¶å¼€å§‹å»ºç«‹ä¸€ä¸ª Kafka + Streaming Producer + Mage Consumer çš„æ•°æ®æ‘„å…¥ç³»ç»Ÿ

---

### ğŸ“Œ é¦–å…ˆï¼Œç†è§£æ•´ä¸ªæµç¨‹

```
Kaggle æ‰¹é‡æ•°æ® (CSV)
    â†“
    â”œâ”€â†’ Kafka Producer (è¯»å– CSVï¼Œé€è¡Œå‘é€åˆ° Kafka)
    â”‚       â†“
    â”‚   Kafka Topic: "supply_chain_data"
    â”‚       â†“
    â””â”€â†’ Mage Consumer (ä» Kafka æ¶ˆè´¹æ•°æ®ï¼Œå†™å…¥ GCS)
            â†“
        GCS Bucket: raw_streaming/*.parquet
```

## â“ é—®é¢˜ä¸€ï¼šåŸå§‹æ•°æ®åº”è¯¥ä¿å­˜åœ¨å“ªé‡Œï¼Ÿ

### ç­”æ¡ˆï¼š

**ä½ åº”è¯¥æŠŠåŸå§‹ CSV æ–‡ä»¶å­˜æ”¾åœ¨é¡¹ç›®çš„æœ¬åœ°ç›®å½•ä¸­**

```
supply_chain_de_study/
â”œâ”€â”€ data/                              # â† åŸå§‹æ•°æ®æ”¾è¿™é‡Œ
â”‚   â”œâ”€â”€ DataCoSupplyChainDataset.csv  
â”‚   â”œâ”€â”€ DescriptionDataCoSupplyChain.csv
â”‚   â””â”€â”€ tokenized_access_logs.csv
â”œâ”€â”€ producer.py                        # Kafka producer è„šæœ¬
â”œâ”€â”€ consumer.py                        # Kafka consumer è„šæœ¬
â”œâ”€â”€ config.py                          # é…ç½®æ–‡ä»¶
â”œâ”€â”€ docker-compose-kafka.yml           # Kafka Docker é…ç½®
â”œâ”€â”€ docker-compose-mage.yml            # Mage Docker é…ç½®
â””â”€â”€ docker-compose-postgres.yml        # PostgreSQL Docker é…ç½®
```

### ä¸ºä»€ä¹ˆåœ¨æœ¬åœ°ï¼Ÿ

- **å¼€å‘é˜¶æ®µ**ï¼šä¾¿äºå¿«é€Ÿæµ‹è¯•å’Œè°ƒè¯•
- **CSV æ–‡ä»¶å°**ï¼šä¾›åº”é“¾æ•°æ®é€šå¸¸ < 1GBï¼Œå¯ä»¥æœ¬åœ°å­˜å‚¨
- **é¿å…ç½‘ç»œæˆæœ¬**ï¼šä¸éœ€è¦é¢‘ç¹ä»äº‘ç«¯ä¸‹è½½
- **æµç¨‹æ¨¡æ‹Ÿ**ï¼šæ¨¡æ‹ŸçœŸå®çš„æµå¼æ•°æ®æ¥æºï¼ˆæ¯”å¦‚æ—¥å¿—æ–‡ä»¶ã€æ•°æ®åº“å¯¼å‡ºï¼‰

### æ•°æ®æ¥æºï¼ˆKaggleï¼‰

```bash
# ä½ åº”è¯¥ä» Kaggle ä¸‹è½½æ•°æ®ï¼š
# https://www.kaggle.com/datasets/shashwatwork/dataco-smart-supply-chain-for-big-data-analysis

# ç„¶åæŠŠ CSV æ”¾åœ¨ supply_chain_de_study/data/ ç›®å½•ä¸‹
```

---

## â“ é—®é¢˜äºŒï¼šæ¶‰åŠ PostgreSQL å’Œ GCS å—ï¼Ÿ

### ç­”æ¡ˆï¼š**éƒ¨åˆ†æ¶‰åŠ**


| ç»„ä»¶           | æ˜¯å¦éœ€è¦  | ä½œç”¨                                         |
| ---------------- | ----------- | ---------------------------------------------- |
| **PostgreSQL** | âœ… éœ€è¦   | Mage çš„å…ƒæ•°æ®å­˜å‚¨ï¼ˆpipeline é…ç½®ã€æ‰§è¡Œå†å²ï¼‰ |
| **GCS**        | âœ… éœ€è¦   | å­˜å‚¨æ¶ˆè´¹åçš„ parquet æ–‡ä»¶                    |
| **GPS åæ ‡**   | âœ… æœ‰æ•°æ® | CSV ä¸­åŒ…å«çº¬åº¦/ç»åº¦å­—æ®µï¼Œä½†ä¸ç‰¹æ®Šå¤„ç†        |

### è¯¦ç»†è¯´æ˜ï¼š

#### PostgreSQL çš„ä½œç”¨

```
Mage éœ€è¦ä¸€ä¸ªæ•°æ®åº“æ¥å­˜å‚¨ï¼š
â”œâ”€ Pipeline é…ç½®
â”œâ”€ æ‰§è¡Œæ—¥å¿—
â”œâ”€ è§¦å‘å™¨ä¿¡æ¯
â”œâ”€ å—ï¼ˆBlocksï¼‰çš„é…ç½®
â””â”€ æ¶ˆè´¹è€…çš„åç§»é‡ï¼ˆoffset trackingï¼‰
```

**ä½†æ˜¯**ï¼šè¿™æ˜¯ Mage çš„å†…éƒ¨éœ€æ±‚ï¼Œä½ ä¸éœ€è¦æ‰‹åŠ¨æ“ä½œå®ƒã€‚Docker Compose ä¼šè‡ªåŠ¨å¯åŠ¨ã€‚

#### GCS çš„ä½œç”¨

```
Mage Consumer å°†æ•°æ®å­˜å‚¨åˆ° GCSï¼š
gs://your-bucket-name/raw_streaming/*.parquet
    â†“
ä¾›ç»™ä¸‹ä¸€é˜¶æ®µï¼ˆSpark Transformï¼‰ä½¿ç”¨
```

**ä½†æ˜¯**ï¼šå¦‚æœä½ è¿˜æ²¡æœ‰ GCP è´¦æˆ·ï¼Œä½ å¯ä»¥å…ˆ**æœ¬åœ°æµ‹è¯•**ï¼Œä½¿ç”¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿæ›¿ä»£ GCSã€‚

---

## â“ é—®é¢˜ä¸‰ï¼šèƒ½ç”¨ Airflow æ›¿ä»£ Mage å—ï¼Ÿ

### ç­”æ¡ˆï¼š**ç†è®ºä¸Šå¯ä»¥ï¼Œä½†ä¸æ¨èã€‚åŸå› å¦‚ä¸‹ï¼š**


| ç‰¹æ€§           | Mage             | Airflow         |
| ---------------- | ------------------ | ----------------- |
| **å­¦ä¹ æ›²çº¿**   | ğŸŸ¢ é™¡å³­          | ğŸ”´ æ›´é™¡         |
| **Kafka é›†æˆ** | ğŸŸ¢ åŸç”Ÿæ”¯æŒ      | ğŸŸ¡ éœ€è¦æ’ä»¶     |
| **å®æ—¶æµå¤„ç†** | ğŸŸ¢ å†…ç½® Streamer | ğŸ”´ ä¸æ˜¯è®¾è®¡ç›®æ ‡ |
| **é…ç½®æ–¹å¼**   | ğŸŸ¢ UI + YAML     | ğŸ”´ åªæœ‰ Python  |
| **å¯åŠ¨é€Ÿåº¦**   | ğŸŸ¢ å¿«            | ğŸ”´ æ…¢           |
| **èµ„æºå ç”¨**   | ğŸŸ¢ ä½ (~500MB)   | ğŸ”´ é«˜ (~2GB)    |

### Mage vs Airflow çš„åŒºåˆ«

**Mage çš„è®¾è®¡**ï¼š

```python
# Mage æ–¹å¼ï¼šç¼–å†™ä¸€ä¸ªæ•°æ®ç®¡é“ï¼Œä¸‰ä¸ªå—ï¼ˆBlocksï¼‰
@loader                    # 1. åŠ è½½å— - ä» Kafka è¯»å–
def load_from_kafka():
    # æ¶ˆè´¹ Kafka æ•°æ®
    pass

@transformer               # 2. è½¬æ¢å— - æ•°æ®å¤„ç†
def transform_data(data):
    # æ¸…æ´—ã€è½¬æ¢
    pass

@exporter                  # 3. å¯¼å‡ºå— - å†™å…¥ GCS
def export_to_gcs(data):
    # å­˜å‚¨æ•°æ®
    pass
```

**Airflow çš„æ–¹å¼**ï¼š

```python
# Airflow æ–¹å¼ï¼šç¼–å†™ä¸€ä¸ª DAGï¼Œå¤šä¸ª Task
from airflow import DAG
from airflow.operators.python import PythonOperator

def kafka_consumer_task():
    pass

def transform_task():
    pass

def gcs_exporter_task():
    pass

dag = DAG('supply_chain_ingestion')
load >> transform >> export
```

### å¦‚æœä½ åšæŒç”¨ Airflowï¼Ÿ

**éœ€è¦åšä»¥ä¸‹æ”¹é€ **ï¼š

1. æ·»åŠ  Kafka æ’ä»¶ï¼š`apache-airflow-providers-apache-kafka`
2. å¤„ç† streaming + æ‰¹å¤„ç†çš„æ··åˆæ¨¡å¼ï¼ˆAirflow ä¸æ“…é•¿æµå¤„ç†ï¼‰
3. ç¼–å†™æ›´å¤šçš„è‡ªå®šä¹‰ä»£ç 
4. **è€—æ—¶ä¼šå¢åŠ  3-5 å€**

**å»ºè®®**ï¼šç°åœ¨å­¦ Mageï¼Œå®ƒæ›´é€‚åˆè¿™ä¸ªé¡¹ç›®ã€‚Airflow æ˜¯ batch å¤„ç†çš„ç‹è€…ï¼Œä¸æ˜¯æµå¤„ç†ã€‚

---

## âœ… å®Œæ•´çš„ Setup æ­¥éª¤ï¼ˆä»é›¶å¼€å§‹ï¼‰

### ç¬¬ä¸€æ­¥ï¼šé¡¹ç›®ç»“æ„åˆå§‹åŒ–

```bash
cd /Users/liceline/Documents/study_material/data_engineer/project_study/supply_chain/supply_chain_de_study

# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p data
mkdir -p configs
mkdir -p scripts
mkdir -p docker
```

### ç¬¬äºŒæ­¥ï¼šä¸‹è½½å’Œå‡†å¤‡æ•°æ®

```bash
# ä» Kaggle ä¸‹è½½æ•°æ®
# https://www.kaggle.com/datasets/shashwatwork/dataco-smart-supply-chain-for-big-data-analysis

# å‡è®¾ä½ å·²ç»ä¸‹è½½ï¼ŒæŠŠ CSV æ”¾åœ¨ï¼š
cp /path/to/DataCoSupplyChainDataset.csv ./data/
```

### ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºé…ç½®æ–‡ä»¶

**æ–‡ä»¶ï¼šsupply_chain_de_study/config.py**

```python
# Kafka é…ç½®
KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'  # æœ¬åœ°å¼€å‘
# KAFKA_BOOTSTRAP_SERVERS = 'kafka:29092'  # Docker å†…éƒ¨
KAFKA_TOPIC = 'supply_chain_data'

# PostgreSQL é…ç½®ï¼ˆMage å…ƒæ•°æ®å­˜å‚¨ï¼‰
POSTGRES_HOST = 'localhost'
POSTGRES_PORT = 5432
POSTGRES_USER = 'postgres'
POSTGRES_PASSWORD = 'postgres'
POSTGRES_DB = 'mage_db'

# GCS é…ç½®ï¼ˆå¯é€‰ï¼ŒåˆæœŸä½¿ç”¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼‰
GCS_PROJECT_ID = 'your-gcp-project'
GCS_BUCKET = 'your-bucket-name'
GCS_PATH = 'raw_streaming'

# CSV æ–‡ä»¶è·¯å¾„
CSV_FILE_PATH = './data/DataCoSupplyChainDataset.csv'

# ç”Ÿäº§å»¶è¿Ÿï¼ˆç§’ï¼‰- ç”¨äºæ¨¡æ‹Ÿæµå¼æ•°æ®
PRODUCER_DELAY = 0.5  # æ¯æ¡è®°å½•å»¶è¿Ÿ 0.5 ç§’
```

### ç¬¬å››æ­¥ï¼šåˆ›å»º Kafka Producerï¼ˆç”Ÿäº§è€…ï¼‰

**æ–‡ä»¶ï¼šsupply_chain_de_study/producer.py**

```python
from confluent_kafka import Producer
import json
import csv
import time
from config import KAFKA_BOOTSTRAP_SERVERS, KAFKA_TOPIC, CSV_FILE_PATH, PRODUCER_DELAY

def read_csv_rows(file_path):
    """ä» CSV è¯»å–æ¯ä¸€è¡Œæ•°æ®"""
    with open(file_path, 'r', encoding='utf-8-sig') as file:
        reader = csv.DictReader(file)
        for row in reader:
            yield row

def delivery_report(err, msg):
    """æ¶ˆæ¯å‘é€å›è°ƒå‡½æ•°"""
    if err is not None:
        print(f'âŒ æ¶ˆæ¯å‘é€å¤±è´¥: {err}')
    else:
        print(f'âœ… æ¶ˆæ¯å·²å‘é€åˆ° topic: {msg.topic()}, '
              f'åˆ†åŒº: {msg.partition()}, åç§»é‡: {msg.offset()}')

def produce_streaming_data():
    """ä¸»ç”Ÿäº§å‡½æ•°"""
    # åˆ›å»º Kafka producer
    producer = Producer({
        'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS,
        'client.id': 'supply_chain_producer'
    })
  
    print(f"ğŸš€ å¼€å§‹ä» {CSV_FILE_PATH} ç”Ÿäº§æ•°æ®åˆ° Kafka topic: {KAFKA_TOPIC}")
  
    try:
        row_count = 0
        for row in read_csv_rows(CSV_FILE_PATH):
            # å°†æ¯ä¸€è¡Œè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
            json_row = json.dumps(row)
          
            # å‘é€åˆ° Kafka
            producer.produce(
                KAFKA_TOPIC,
                value=json_row.encode('utf-8'),
                callback=delivery_report
            )
          
            row_count += 1
          
            # æ¨¡æ‹Ÿæµå¼æ•°æ®çš„å»¶è¿Ÿ
            time.sleep(PRODUCER_DELAY)
          
            # æ¯ 100 æ¡è®°å½•è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if row_count % 100 == 0:
                print(f"ğŸ“Š å·²ç”Ÿäº§ {row_count} æ¡è®°å½•")
                producer.flush()  # å®šæœŸåˆ·æ–°
  
    except KeyboardInterrupt:
        print("\nâ¸ï¸  ç”¨æˆ·ä¸­æ–­ç”Ÿäº§")
  
    finally:
        # ç¡®ä¿æ‰€æœ‰æ¶ˆæ¯éƒ½è¢«å‘é€
        producer.flush()
        print(f"âœ… ç”Ÿäº§å®Œæˆï¼Œå…± {row_count} æ¡è®°å½•")

if __name__ == '__main__':
    produce_streaming_data()
```

### ç¬¬äº”æ­¥ï¼šåˆ›å»º Docker Compose æ–‡ä»¶

**æ–‡ä»¶ï¼šsupply_chain_de_study/docker/docker-compose-kafka.yml**

```yaml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: mage_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

**æ–‡ä»¶ï¼šsupply_chain_de_study/docker/docker-compose-mage.yml**

```yaml
version: '3.8'

services:
  mage:
    image: mageai/mageai:latest
    container_name: mage-pipeline
    command: mage start supply_chain_project
    environment:
      POSTGRES_DBNAME: mage_db
      POSTGRES_SCHEMA: public
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
    ports:
      - "6789:6789"
    volumes:
      - ./mage_project:/home/src
    depends_on:
      - postgres
```

### ç¬¬å…­æ­¥ï¼šå¯åŠ¨æœåŠ¡

```bash
# 1. å¯åŠ¨ Kafka + PostgreSQL
docker-compose -f docker/docker-compose-kafka.yml up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 10

# 2. å¯åŠ¨ Mage
docker-compose -f docker/docker-compose-mage.yml up -d

# ç­‰å¾… Mage åˆå§‹åŒ–
sleep 15

# 3. è®¿é—® Mage UI
# æ‰“å¼€æµè§ˆå™¨è®¿é—®ï¼šhttp://localhost:6789
```

### ç¬¬ä¸ƒæ­¥ï¼šåˆ›å»º Mage Pipelineï¼ˆåœ¨ UI ä¸­æˆ–ä»£ç ä¸­ï¼‰

**åœ¨ Mage UI ä¸­ï¼š**

1. è®¿é—® `http://localhost:6789`
2. ç‚¹å‡» "Create Pipeline"
3. é€‰æ‹© "Standard (Batch)" æˆ– "Streaming"
4. æ·»åŠ ä¸‰ä¸ªå—ï¼š
   - **Load Block**ï¼šä» Kafka è¯»å–
   - **Transform Block**ï¼šæ•°æ®å¤„ç†
   - **Export Block**ï¼šå†™å…¥å­˜å‚¨

**æˆ–è€…ç›´æ¥ç¼–å†™ä»£ç ï¼š**

**æ–‡ä»¶ï¼šsupply_chain_de_study/mage_project/pipelines/kafka_to_storage.py**

```python
from mage_ai.orchestration.triggers.api import trigger_pipeline
from mage_ai.io.kafka import KafkaConsumer
import json

@loader
def load_from_kafka():
    """ä» Kafka æ¶ˆè´¹æ•°æ®"""
    consumer = KafkaConsumer(
        topic='supply_chain_data',
        bootstrap_servers='kafka:29092',
        group_id='mage_consumer'
    )
  
    data = []
    for message in consumer:
        row = json.loads(message.value.decode('utf-8'))
        data.append(row)
      
        # æ¯ 1000 æ¡è®°å½•è¿”å›ä¸€æ¬¡
        if len(data) >= 1000:
            break
  
    return data

@transformer
def transform_data(data):
    """æ•°æ®è½¬æ¢å’Œæ¸…æ´—"""
    import pandas as pd
  
    df = pd.DataFrame(data)
  
    # æ•°æ®æ¸…æ´—ç¤ºä¾‹
    df = df.dropna()  # åˆ é™¤ç¼ºå¤±å€¼
    df['timestamp'] = pd.Timestamp.now()
  
    return df

@exporter
def export_to_parquet(df):
    """å¯¼å‡ºä¸º Parquet æ ¼å¼"""
    import pyarrow.parquet as pq
    from datetime import datetime
  
    # æœ¬åœ°å­˜å‚¨è·¯å¾„ï¼ˆåæœŸå¯æ”¹ä¸º GCSï¼‰
    file_path = f'./data/raw_streaming/stream_{datetime.now().strftime("%Y%m%d_%H%M%S")}.parquet'
  
    df.to_parquet(file_path)
    print(f"âœ… æ•°æ®å·²å¯¼å‡ºåˆ° {file_path}")
```

---

## ğŸ”Œ è¿è¡Œæµç¨‹

```bash
# ç»ˆç«¯ 1ï¼šå¯åŠ¨ Kafka
docker-compose -f docker/docker-compose-kafka.yml up

# ç»ˆç«¯ 2ï¼šå¯åŠ¨ Mage
docker-compose -f docker/docker-compose-mage.yml up

# ç»ˆç«¯ 3ï¼šè¿è¡Œ Producerï¼ˆç”Ÿäº§æ•°æ®ï¼‰
python producer.py

# ç»ˆç«¯ 4ï¼šåœ¨ Mage UI ä¸­æ‰‹åŠ¨è§¦å‘ pipeline
# æˆ–é€šè¿‡ Mage API è°ƒç”¨
curl -X POST http://localhost:6789/api/pipeline_runs \
  -H "Content-Type: application/json" \
  -d '{"pipeline_uuid": "your_pipeline_uuid"}'
```

---

## ğŸ“Š éªŒè¯æ•°æ®æµ

```bash
# 1. éªŒè¯ Kafka topic ä¸­æ˜¯å¦æœ‰æ•°æ®
docker exec -it <kafka-container-id> \
  kafka-console-consumer.sh \
  --bootstrap-server kafka:9092 \
  --topic supply_chain_data \
  --from-beginning \
  --max-messages 5

# 2. æŸ¥çœ‹ Mage æ—¥å¿—
docker logs -f <mage-container-id>

# 3. æŸ¥çœ‹ç”Ÿæˆçš„ Parquet æ–‡ä»¶
ls -lh ./data/raw_streaming/
```

---

## âš ï¸ å¸¸è§é—®é¢˜


| é—®é¢˜                  | åŸå›             | è§£å†³                                  |
| ----------------------- | ----------------- | --------------------------------------- |
| Producer è¿æ¥å¤±è´¥     | Kafka æœªå¯åŠ¨    | `docker-compose up` å¹¶ç­‰å¾… 10 ç§’      |
| Mage æ— æ³•è¿æ¥åˆ° Kafka | ç½‘ç»œé—®é¢˜        | æ£€æŸ¥ Docker ç½‘ç»œï¼Œæˆ–æ”¹ç”¨`kafka:29092` |
| Parquet æ–‡ä»¶ä¸ºç©º      | Consumer æœªè¿è¡Œ | ç¡®ä¿ Mage pipeline å·²å¯åŠ¨             |
| å†…å­˜ä¸è¶³              | æ•°æ®é‡è¿‡å¤§      | å‡å° batch å¤§å°æˆ–è°ƒæ•´ Docker å†…å­˜     |

---

## ğŸ¯ æœ¬é˜¶æ®µå…³é”®æ¦‚å¿µæ€»ç»“


| æ¦‚å¿µ            | è§£é‡Š                               |
| ----------------- | ------------------------------------ |
| **Kafka Topic** | æ¶ˆæ¯é˜Ÿåˆ—çš„ä¸»é¢˜ï¼Œåƒä¸€ä¸ªé¢‘é“         |
| **Producer**    | ç”Ÿäº§è€…ï¼Œä» CSV è¯»å–å¹¶å‘é€æ•°æ®      |
| **Consumer**    | æ¶ˆè´¹è€…ï¼Œè¯»å–å¹¶å¤„ç†æ•°æ®             |
| **Offset**      | æ¶ˆè´¹è¿›åº¦ï¼Œè®°å½•è¯»åˆ°å“ªæ¡æ¶ˆæ¯         |
| **Partition**   | ä¸»é¢˜åˆ†åŒºï¼Œç”¨äºå¹¶è¡Œå¤„ç†             |
| **Mage Block**  | ç®¡é“çš„ä¸€ä¸ªæ­¥éª¤ï¼ˆåŠ è½½ã€è½¬æ¢ã€å¯¼å‡ºï¼‰ |

---

## ğŸ“ ä¸‹ä¸€æ­¥

å®Œæˆ Ingestion é˜¶æ®µåï¼Œä½ å°†æœ‰ï¼š

- âœ… Kafka ä¸­çš„å®æ—¶æ•°æ®æµ
- âœ… Parquet æ ¼å¼çš„æœ¬åœ°æ•°æ®æ–‡ä»¶
- âœ… åœ¨ Mage ä¸­è¿è¡Œçš„ ETL ç®¡é“

ä¸‹ä¸€æ­¥ï¼š**Transform é˜¶æ®µ** - ä½¿ç”¨ Spark å¤„ç†å’Œè½¬æ¢æ•°æ®
