# Batch Pipeline æ‰§è¡Œå®Œæ•´æŒ‡å—

## ç›®å½•
- [æ¦‚è¿°](#æ¦‚è¿°)
- [Docker Spark ç¯å¢ƒé…ç½®](#docker-spark-ç¯å¢ƒé…ç½®)
- [æ‰§è¡Œæ­¥éª¤è¯¦è§£](#æ‰§è¡Œæ­¥éª¤è¯¦è§£)
- [æ€§èƒ½ä¼˜åŒ–å»ºè®®](#æ€§èƒ½ä¼˜åŒ–å»ºè®®)
- [å¸¸è§é—®é¢˜æ’æŸ¥](#å¸¸è§é—®é¢˜æ’æŸ¥)

---

## æ¦‚è¿°

æœ¬æ–‡æ¡£è®°å½•äº†è¿è¡Œ `batch_pipeline/data_modeling/transformed_data.py` çš„å®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
- Docker Spark é›†ç¾¤çš„é…ç½®ä¸å¯åŠ¨
- GCS è¿æ¥å™¨çš„é›†æˆ
- Java ç¯å¢ƒé…ç½®
- Pipeline æ‰§è¡Œè¿‡ç¨‹
- æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

**æ‰§è¡Œæ—¶é—´å‚è€ƒ**ï¼šåˆå§‹é…ç½®çº¦ 1 å°æ—¶ï¼ˆåŒ…å«æ•°æ®è¯»å–ã€è½¬æ¢å’Œå†™å…¥ï¼‰

---

## Docker Spark ç¯å¢ƒé…ç½®

### 1. ç›®å½•ç»“æ„
```
docker/spark/
â”œâ”€â”€ cluster-base.Dockerfile       # åŸºç¡€é•œåƒï¼šJava 17 + Python
â”œâ”€â”€ spark-base.Dockerfile         # Spark åŸºç¡€é•œåƒ
â”œâ”€â”€ spark-master.Dockerfile       # Spark Master èŠ‚ç‚¹
â”œâ”€â”€ spark-worker.Dockerfile       # Spark Worker èŠ‚ç‚¹
â”œâ”€â”€ jupyterlab.Dockerfile         # JupyterLab é•œåƒ
â”œâ”€â”€ docker-compose.yaml           # æœåŠ¡ç¼–æ’é…ç½®
â”œâ”€â”€ .env                          # ç¯å¢ƒå˜é‡
â””â”€â”€ jar_files/
    â””â”€â”€ gcs-connector-hadoop3-2.2.5.jar  # GCS è¿æ¥å™¨
```

### 2. é…ç½®æ–‡ä»¶è¯¦è§£

#### 2.1 cluster-base.Dockerfile
**ç›®çš„**ï¼šæä¾›ç»Ÿä¸€çš„åŸºç¡€ç¯å¢ƒï¼ˆJava 17 + Python 3ï¼‰

```dockerfile
ARG java_image_tag=17-jre
FROM eclipse-temurin:${java_image_tag}

ARG shared_workspace=/opt/workspace

RUN mkdir -p ${shared_workspace} && \
    apt-get update -y && \
    apt-get install -y python3 && \
    ln -s /usr/bin/python3 /usr/bin/python && \
    rm -rf /var/lib/apt/lists/*

ENV SHARED_WORKSPACE=${shared_workspace}
ENV JAVA_HOME=/opt/java/openjdk
ENV PATH=${JAVA_HOME}/bin:${PATH}
```

**å…³é”®é…ç½®**ï¼š
- `java_image_tag=17-jre`ï¼šä½¿ç”¨ Java 17ï¼ˆSpark 3.5.0 è¦æ±‚ Java 17ï¼‰
- `JAVA_HOME` å’Œ `PATH`ï¼šç¡®ä¿ Java ç¯å¢ƒæ­£ç¡®é…ç½®

#### 2.2 spark-base.Dockerfile
**ç›®çš„**ï¼šå®‰è£… Apache Spark å¹¶é›†æˆ GCS è¿æ¥å™¨

```dockerfile
FROM cluster-base

ARG spark_version=3.5.0
ARG hadoop_version=3

RUN apt-get update -y && \
    apt-get install -y curl && \
    curl https://archive.apache.org/dist/spark/spark-${spark_version}/spark-${spark_version}-bin-hadoop${hadoop_version}.tgz -o spark.tgz && \
    tar -xf spark.tgz && \
    mv spark-${spark_version}-bin-hadoop${hadoop_version} /usr/bin/ && \
    mkdir /usr/bin/spark-${spark_version}-bin-hadoop${hadoop_version}/logs && \
    rm spark.tgz

ENV SPARK_HOME /usr/bin/spark-${spark_version}-bin-hadoop${hadoop_version}
ENV SPARK_MASTER_HOST spark-master
ENV SPARK_MASTER_PORT 7077
ENV PYSPARK_PYTHON python3

# å¤åˆ¶ GCS è¿æ¥å™¨ JAR åˆ° Spark jars ç›®å½•
COPY jar_files/gcs-connector-hadoop3-2.2.5.jar ${SPARK_HOME}/jars/
```

**å…³é”®é…ç½®**ï¼š
- `spark_version=3.5.0`ï¼šä½¿ç”¨ Spark 3.5.0
- `GCS è¿æ¥å™¨`ï¼šæ”¯æŒ `gs://` åè®®è¯»å†™ Google Cloud Storage

#### 2.3 jupyterlab.Dockerfile
**ç›®çš„**ï¼šåˆ›å»ºå¸¦æœ‰ PySpark å’Œ JupyterLab çš„å¼€å‘ç¯å¢ƒ

```dockerfile
FROM cluster-base

ARG spark_version=3.5.0
ARG jupyterlab_version=3.6.1

RUN apt-get update -y && \
    apt-get install -y python3-pip && \
    pip3 install --break-system-packages wget pyspark==${spark_version} jupyterlab==${jupyterlab_version}

# å¤åˆ¶ GCS è¿æ¥å™¨ JAR åˆ° PySpark jars ç›®å½•ï¼Œæ”¯æŒ gs:// è¯»å†™
COPY jar_files/gcs-connector-hadoop3-2.2.5.jar /usr/local/lib/python3.12/dist-packages/pyspark/jars/

EXPOSE 8888
WORKDIR ${SHARED_WORKSPACE}
CMD jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=
```

**å…³é”®é…ç½®**ï¼š
- PySpark ç‰ˆæœ¬ä¸ Spark ç‰ˆæœ¬ä¸€è‡´ï¼ˆ3.5.0ï¼‰
- GCS è¿æ¥å™¨åŒæ—¶å¤åˆ¶åˆ° PySpark çš„ jars ç›®å½•
- å·¥ä½œç›®å½•æŒ‚è½½åˆ° `/opt/workspace`

#### 2.4 docker-compose.yaml
**ç›®çš„**ï¼šç¼–æ’å¤šå®¹å™¨æœåŠ¡

```yaml
version: "3.6"

volumes:
  spark-logs:
    driver: local

networks:
  default:
    name: ${PROJECT_NAME}-network
    external: true

services:
  jupyterlab:
    build:
      context: .
      dockerfile: jupyterlab.Dockerfile
    image: supply-chain-jupyterlab
    container_name: ${PROJECT_NAME}-jupyterlab
    volumes:
      - /Users/liceline/.../supply_chain_de_study:/opt/workspace
    environment:
      - PROJECT_ROOT=/opt/workspace
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/workspace/keys/gcp-cred.json
      - GCP_PROJECT_ID=stellar-stream-485314-p0
      - GCS_BUCKET=supply-chain-data-bucket-485314
    ports:
      - 8888:8888

  spark-master:
    build:
      context: .
      dockerfile: spark-master.Dockerfile
    image: supply-chain-spark-master
    container_name: ${PROJECT_NAME}-spark-master
    volumes:
      - /Users/liceline/.../supply_chain_de_study:/opt/workspace
      - spark-logs:/opt/spark-logs
    environment:
      SPARK_LOCAL_IP: spark-master
      PROJECT_ROOT: /opt/workspace
      GOOGLE_APPLICATION_CREDENTIALS: /opt/workspace/keys/gcp-cred.json
    ports:
      - 18080:8080  # Spark Master Web UIï¼ˆæ”¹ä¸º 18080 é¿å…ç«¯å£å†²çªï¼‰
      - 7077:7077   # Spark Master Port

  spark-worker-1:
    build:
      context: .
      dockerfile: spark-worker.Dockerfile
    image: supply-chain-spark-worker
    container_name: ${PROJECT_NAME}-spark-worker-1
    depends_on:
      - spark-master
    volumes:
      - /Users/liceline/.../supply_chain_de_study:/opt/workspace
      - spark-logs:/opt/spark-logs
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=4g
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/workspace/keys/gcp-cred.json
    ports:
      - 8083:8081
```

**å…³é”®é…ç½®**ï¼š
- **å¤–éƒ¨ç½‘ç»œ**ï¼š`supply-chain-network`ï¼ˆéœ€è¦é¢„å…ˆåˆ›å»ºï¼‰
- **å·æŒ‚è½½**ï¼šæœ¬åœ°é¡¹ç›®ç›®å½•æŒ‚è½½åˆ°å®¹å™¨ `/opt/workspace`
- **GCP å‡­è¯**ï¼šé€šè¿‡ç¯å¢ƒå˜é‡å’Œå·æŒ‚è½½ä¼ é€’
- **ç«¯å£æ˜ å°„**ï¼š
  - `8888`ï¼šJupyterLab
  - `18080`ï¼šSpark Master Web UI
  - `7077`ï¼šSpark Master æœåŠ¡ç«¯å£
  - `8083`ï¼šSpark Worker Web UI

---

## æ‰§è¡Œæ­¥éª¤è¯¦è§£

### æ­¥éª¤ 1ï¼šåˆ›å»º Docker ç½‘ç»œ
```bash
docker network create supply-chain-network
```

**ç›®çš„**ï¼šåˆ›å»ºä¾›æ‰€æœ‰å®¹å™¨ä½¿ç”¨çš„å¤–éƒ¨ç½‘ç»œ

---

### æ­¥éª¤ 2ï¼šæ„å»º Docker é•œåƒ
```bash
cd /path/to/docker/spark

docker compose \
  -f docker-compose.yaml \
  --env-file .env \
  build
```

**æ„å»ºé¡ºåº**ï¼š
1. `cluster-base`ï¼šåŸºç¡€é•œåƒï¼ˆJava 17 + Pythonï¼‰
2. `spark-base`ï¼šSpark åŸºç¡€é•œåƒ + GCS è¿æ¥å™¨
3. `spark-master`ã€`spark-worker-1`ï¼šç»§æ‰¿è‡ª spark-base
4. `jupyterlab`ï¼šç»§æ‰¿è‡ª cluster-baseï¼Œç‹¬ç«‹å®‰è£… PySpark

**é¢„è®¡æ—¶é—´**ï¼š5-10 åˆ†é’Ÿ

---

### æ­¥éª¤ 3ï¼šå¯åŠ¨å®¹å™¨é›†ç¾¤
```bash
docker compose \
  -f docker-compose.yaml \
  --env-file .env \
  up -d
```

**å¯åŠ¨é¡ºåº**ï¼š
1. `jupyterlab`
2. `spark-master`
3. `spark-worker-1`ï¼ˆä¾èµ– spark-masterï¼‰

**éªŒè¯å¯åŠ¨**ï¼š
```bash
docker ps
# åº”è¯¥çœ‹åˆ° 3 ä¸ªå®¹å™¨éƒ½åœ¨è¿è¡Œ

# è®¿é—® Spark Master Web UI
open http://localhost:18080
```

---

### æ­¥éª¤ 4ï¼šè¿è¡Œ Batch Pipeline

#### 4.1 Pipeline ä»£ç å…³é”®é…ç½®

**æ–‡ä»¶**ï¼š`batch_pipeline/data_modeling/transformed_data.py`

```python
# Spark Session é…ç½®
spark = SparkSession.builder \
    .appName('supply-chain-batch-pipeline') \
    .config("spark.jars", jar_file_path) \
    .config("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS") \
    .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
    .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
    .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", credentials_path) \
    .config("spark.hadoop.google.cloud.project.id", GCP_PROJECT_ID) \
    .getOrCreate()
```

**å…³é”®ç‚¹**ï¼š
- GCS æ–‡ä»¶ç³»ç»Ÿå®ç°ç±»å¿…é¡»åœ¨ SparkSession åˆ›å»º**ä¹‹å‰**é…ç½®
- æœåŠ¡è´¦å·è®¤è¯æ–‡ä»¶è·¯å¾„å¿…é¡»æ­£ç¡®

#### 4.2 æ‰§è¡Œå‘½ä»¤
```bash
docker exec supply-chain-jupyterlab \
  python /opt/workspace/batch_pipeline/data_modeling/transformed_data.py
```

#### 4.3 æ‰§è¡Œæµç¨‹
1. **åˆå§‹åŒ– SparkSession**ï¼ˆ30ç§’ï¼‰
   - åŠ è½½ GCS è¿æ¥å™¨ JAR
   - é…ç½® Hadoop GCS æ–‡ä»¶ç³»ç»Ÿ
   - å»ºç«‹ä¸ GCP çš„è®¤è¯è¿æ¥

2. **è¯»å– GCS æ•°æ®**ï¼ˆ10-20 åˆ†é’Ÿï¼‰
   - è·¯å¾„ï¼š`gs://supply-chain-data-bucket-485314/raw_streaming/*.parquet`
   - Stage 2-4ï¼šè¯»å–å¹¶è§£æ Parquet æ–‡ä»¶

3. **æ•°æ®è½¬æ¢**ï¼ˆ5-10 åˆ†é’Ÿï¼‰
   - åˆ›å»º 7 ä¸ªç»´åº¦è¡¨ï¼š
     - `customer_dimension`
     - `product_dimension`
     - `location_dimension`
     - `order_fact`
     - `shipping_dimension`
     - `department_dimension`
     - `metadata_dimension`

4. **å†™å…¥ GCS**ï¼ˆ20-30 åˆ†é’Ÿï¼‰
   - è·¯å¾„ï¼š`gs://supply-chain-data-bucket-485314/transformed_data/`
   - Stage 5-8ï¼šå¹¶è¡Œå†™å…¥ Parquet æ–‡ä»¶
   - å‹ç¼©æ ¼å¼ï¼šSnappy

**æ€»è€—æ—¶**ï¼šçº¦ 35-60 åˆ†é’Ÿ

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å¢åŠ å¹¶è¡Œåº¦

#### 1.1 å¢åŠ  Spark Worker æ•°é‡
**å½“å‰é…ç½®**ï¼š1 ä¸ª Workerï¼Œ2 coresï¼Œ4GB å†…å­˜

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼šåœ¨ `docker-compose.yaml` æ·»åŠ æ›´å¤š Worker

```yaml
spark-worker-2:
  build:
    context: .
    dockerfile: spark-worker.Dockerfile
  image: supply-chain-spark-worker
  container_name: ${PROJECT_NAME}-spark-worker-2
  depends_on:
    - spark-master
  volumes:
    - /Users/liceline/.../supply_chain_de_study:/opt/workspace
  environment:
    - SPARK_WORKER_CORES=2
    - SPARK_WORKER_MEMORY=4g
  ports:
    - 8084:8081
```

**é¢„æœŸæå‡**ï¼šè¯»å†™é€Ÿåº¦æå‡ 30-50%

---

#### 1.2 è°ƒæ•´åˆ†åŒºæ•°é‡
**å½“å‰é…ç½®**ï¼š`config.py` ä¸­ `partition_count = 4`

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š
```python
# config.py
partition_count = 8  # å¢åŠ åˆ° 8 æˆ–æ›´å¤š

# æˆ–æ ¹æ®æ•°æ®é‡åŠ¨æ€è°ƒæ•´
# æ¨èï¼špartition_count = worker_count * cores_per_worker * 2
# ä¾‹å¦‚ï¼š2 workers * 2 cores * 2 = 8
```

**ä¿®æ”¹ä»£ç **ï¼š
```python
# transformed_data.py
def write_to_gcs(dataframes, output_path):
    for name, dataframe in dataframes.items():
        output_file = output_path + name
        # å¢åŠ åˆ†åŒºæ•°ä»¥æé«˜å¹¶è¡Œå†™å…¥
        target_df = dataframe.repartition(partition_count)
        target_df.write.mode("overwrite").option("compression", "snappy").parquet(output_file)
```

**é¢„æœŸæå‡**ï¼šå†™å…¥é€Ÿåº¦æå‡ 20-40%

---

### 2. ä¼˜åŒ– Spark é…ç½®

#### 2.1 åœ¨ SparkSession ä¸­æ·»åŠ æ€§èƒ½é…ç½®
```python
spark = SparkSession.builder \
    .appName('supply-chain-batch-pipeline') \
    .config("spark.jars", jar_file_path) \
    .config("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS") \
    .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
    .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
    .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", credentials_path) \
    .config("spark.hadoop.google.cloud.project.id", GCP_PROJECT_ID) \
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    .config("spark.sql.shuffle.partitions", "200")  # é»˜è®¤ 200ï¼Œå¯æ ¹æ®æ•°æ®é‡è°ƒæ•´
    .config("spark.default.parallelism", "16")      # é»˜è®¤å¹¶è¡Œä»»åŠ¡æ•°
    .config("spark.sql.adaptive.enabled", "true")   # å¯ç”¨è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true")  # è‡ªåŠ¨åˆå¹¶å°åˆ†åŒº
    .config("spark.executor.memory", "4g")          # Executor å†…å­˜
    .config("spark.driver.memory", "2g")            # Driver å†…å­˜
    .config("spark.memory.fraction", "0.8")         # JVM å †å†…å­˜çš„ 80% ç”¨äº Spark
    .getOrCreate()
```

---

#### 2.2 GCS è¿æ¥å™¨ä¼˜åŒ–
```python
# åœ¨ Hadoop é…ç½®ä¸­æ·»åŠ  GCS ç¼“å†²åŒºè®¾ç½®
hadoop_conf = spark._jsc.hadoopConfiguration()
hadoop_conf.set("fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
hadoop_conf.set("fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
hadoop_conf.set("fs.gs.auth.service.account.json.keyfile", credentials_path)
hadoop_conf.set("fs.gs.auth.service.account.enable", "true")

# æ€§èƒ½ä¼˜åŒ–
hadoop_conf.set("fs.gs.block.size", "134217728")  # 128 MB block size
hadoop_conf.set("fs.gs.inputstream.buffer.size", "8388608")  # 8 MB è¯»å–ç¼“å†²åŒº
hadoop_conf.set("fs.gs.outputstream.buffer.size", "8388608")  # 8 MB å†™å…¥ç¼“å†²åŒº
hadoop_conf.set("fs.gs.outputstream.upload.chunk.size", "67108864")  # 64 MB ä¸Šä¼ å—å¤§å°
```

**é¢„æœŸæå‡**ï¼šGCS è¯»å†™é€Ÿåº¦æå‡ 15-25%

---

### 3. æ•°æ®è¯»å–ä¼˜åŒ–

#### 3.1 å¯ç”¨åˆ—è£å‰ªï¼ˆColumn Pruningï¼‰
**å½“å‰å®ç°**ï¼šå·²ç»ä½¿ç”¨åˆ—é€‰æ‹©

```python
def extract_columns(df, columns_to_extract):
    available_cols = [col_name for col_name in columns_to_extract if col_name in df.columns]
    return df.select(*available_cols)  # âœ… åªè¯»å–éœ€è¦çš„åˆ—
```

**è¿›ä¸€æ­¥ä¼˜åŒ–**ï¼šåœ¨è¯»å–æ—¶å°±æŒ‡å®šåˆ—
```python
# å¦‚æœçŸ¥é“æ‰€æœ‰éœ€è¦çš„åˆ—ï¼Œå¯ä»¥åœ¨è¯»å–æ—¶æŒ‡å®š
all_required_columns = list(set(
    customer_columns + product_columns + location_columns + 
    order_columns + shipping_columns + department_columns + metadata_columns
))

df_raw = spark.read.parquet(input_path + '*').select(*all_required_columns)
```

---

#### 3.2 å¯ç”¨è°“è¯ä¸‹æ¨ï¼ˆPredicate Pushdownï¼‰
å¦‚æœåªéœ€è¦éƒ¨åˆ†æ•°æ®ï¼Œåœ¨è¯»å–æ—¶è¿‡æ»¤ï¼š

```python
# ä¾‹å¦‚ï¼šåªå¤„ç†ç‰¹å®šæ—¥æœŸèŒƒå›´çš„æ•°æ®
df_raw = spark.read.parquet(input_path + '*') \
    .filter(col("Order date") >= "2020-01-01") \
    .filter(col("Order date") <= "2023-12-31")
```

---

### 4. å†™å…¥ä¼˜åŒ–

#### 4.1 é¿å… `coalesce(1)`
**é—®é¢˜**ï¼š`coalesce(1)` ä¼šå¼ºåˆ¶æ‰€æœ‰æ•°æ®å†™å…¥å•ä¸ªæ–‡ä»¶ï¼Œé€ æˆç“¶é¢ˆ

**å½“å‰å®ç°**ï¼šâœ… å·²ä¼˜åŒ–
```python
# ä½¿ç”¨ repartition è€Œä¸æ˜¯ coalesce(1)
target_df = dataframe.repartition(partition_count)
target_df.write.mode("overwrite").option("compression", "snappy").parquet(output_file)
```

---

#### 4.2 è°ƒæ•´å‹ç¼©ç®—æ³•
**å½“å‰**ï¼šSnappyï¼ˆå¿«é€Ÿä½†å‹ç¼©ç‡ä½ï¼‰

**å¯é€‰æ–¹æ¡ˆ**ï¼š
- `gzip`ï¼šé«˜å‹ç¼©ç‡ï¼Œæ…¢é€Ÿï¼ˆèŠ‚çœå­˜å‚¨æˆæœ¬ï¼‰
- `lz4`ï¼šå¹³è¡¡
- `zstd`ï¼šé«˜å‹ç¼©ç‡ï¼Œè¾ƒå¿«ï¼ˆæ¨èï¼‰

```python
dataframe.write.mode("overwrite").option("compression", "zstd").parquet(output_file)
```

---

### 5. èµ„æºé…ç½®ä¼˜åŒ–

#### 5.1 å¢åŠ  Worker å†…å­˜å’Œ CPU
**å½“å‰é…ç½®**ï¼š
```yaml
environment:
  - SPARK_WORKER_CORES=2
  - SPARK_WORKER_MEMORY=4g
```

**æ¨èé…ç½®**ï¼ˆå¦‚æœç¡¬ä»¶å…è®¸ï¼‰ï¼š
```yaml
environment:
  - SPARK_WORKER_CORES=4      # å¢åŠ åˆ° 4 æ ¸
  - SPARK_WORKER_MEMORY=8g    # å¢åŠ åˆ° 8GB
```

---

#### 5.2 Docker èµ„æºé™åˆ¶
ç¡®ä¿ Docker Desktop æœ‰è¶³å¤Ÿèµ„æºï¼š
- **CPU**ï¼šè‡³å°‘ 4 æ ¸
- **å†…å­˜**ï¼šè‡³å°‘ 8GB
- **ç£ç›˜**ï¼šè‡³å°‘ 20GB

**è®¾ç½®è·¯å¾„**ï¼šDocker Desktop â†’ Preferences â†’ Resources

---

### 6. ä½¿ç”¨ Cloud Dataprocï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

å¯¹äºå¤§è§„æ¨¡æ•°æ®å¤„ç†ï¼Œè€ƒè™‘ä½¿ç”¨ Google Cloud Dataprocï¼š

```bash
# åˆ›å»º Dataproc é›†ç¾¤
gcloud dataproc clusters create supply-chain-cluster \
  --region=europe-west4 \
  --zone=europe-west4-a \
  --master-machine-type=n1-standard-4 \
  --master-boot-disk-size=50GB \
  --num-workers=3 \
  --worker-machine-type=n1-standard-4 \
  --worker-boot-disk-size=50GB \
  --image-version=2.1-debian11 \
  --project=stellar-stream-485314-p0

# æäº¤ä½œä¸š
gcloud dataproc jobs submit pyspark \
  /opt/workspace/batch_pipeline/data_modeling/transformed_data.py \
  --cluster=supply-chain-cluster \
  --region=europe-west4
```

**é¢„æœŸæå‡**ï¼šå¤„ç†æ—¶é—´ä» 1 å°æ—¶ç¼©çŸ­åˆ° 10-15 åˆ†é’Ÿ

---

## æ€§èƒ½ä¼˜åŒ–æ€»ç»“

| ä¼˜åŒ–æ–¹æ¡ˆ | éš¾åº¦ | é¢„æœŸæå‡ | æˆæœ¬ |
|---------|------|---------|------|
| å¢åŠ  Spark Worker æ•°é‡ | ç®€å• | 30-50% | æœ¬åœ°èµ„æº |
| è°ƒæ•´åˆ†åŒºæ•°é‡ | ç®€å• | 20-40% | æ—  |
| ä¼˜åŒ– Spark é…ç½® | ä¸­ç­‰ | 15-25% | æ—  |
| ä¼˜åŒ– GCS è¿æ¥å™¨é…ç½® | ç®€å• | 15-25% | æ—  |
| å¢åŠ  Worker èµ„æº | ç®€å• | 20-30% | æœ¬åœ°èµ„æº |
| ä½¿ç”¨ Cloud Dataproc | å¤æ‚ | 80-90% | GCP è´¹ç”¨ |

**å»ºè®®ä¼˜å…ˆçº§**ï¼š
1. âœ… è°ƒæ•´åˆ†åŒºæ•°é‡ï¼ˆå·²å®ç°ï¼‰
2. å¢åŠ  Spark Worker æ•°é‡ï¼ˆ1 â†’ 2-3ï¼‰
3. ä¼˜åŒ– Spark å’Œ GCS é…ç½®
4. å¢åŠ  Worker èµ„æºï¼ˆ2 cores â†’ 4 coresï¼Œ4GB â†’ 8GBï¼‰
5. è€ƒè™‘ä½¿ç”¨ Cloud Dataprocï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

**ç»¼åˆä¼˜åŒ–åé¢„æœŸ**ï¼š
- å½“å‰ï¼š60 åˆ†é’Ÿ
- ä¼˜åŒ–åï¼š15-25 åˆ†é’Ÿ
- Dataprocï¼š10-15 åˆ†é’Ÿ

---

## å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜ 1ï¼šFileNotFoundException: gs://.../* ä¸å­˜åœ¨
**åŸå› **ï¼šGCS è·¯å¾„ä¸ºç©ºæˆ–ä¸å­˜åœ¨

**æ’æŸ¥**ï¼š
```bash
# éªŒè¯ GCS è·¯å¾„
gsutil ls gs://supply-chain-data-bucket-485314/raw_streaming/

# æ£€æŸ¥æœåŠ¡è´¦å·æƒé™
gcloud projects get-iam-policy stellar-stream-485314-p0
```

**è§£å†³**ï¼š
- ç¡®ä¿æµå¤„ç† Pipeline å·²ç”Ÿæˆæ•°æ®
- éªŒè¯æœåŠ¡è´¦å·æœ‰ `roles/storage.objectViewer` æƒé™

---

### é—®é¢˜ 2ï¼šJava version mismatch
**é”™è¯¯**ï¼š`UnsupportedClassVersionError: class file version 61.0`

**åŸå› **ï¼šJava ç‰ˆæœ¬è¿‡ä½ï¼ˆéœ€è¦ Java 17ï¼‰

**è§£å†³**ï¼š
```dockerfile
# cluster-base.Dockerfile ä¸­æŒ‡å®š Java 17
ARG java_image_tag=17-jre
FROM eclipse-temurin:${java_image_tag}
```

---

### é—®é¢˜ 3ï¼šWrong FS: gs://, expected: file:///
**åŸå› **ï¼šGCS æ–‡ä»¶ç³»ç»Ÿæœªæ­£ç¡®é…ç½®

**è§£å†³**ï¼š
1. ç¡®ä¿ GCS è¿æ¥å™¨ JAR å·²å¤åˆ¶åˆ° Spark jars ç›®å½•
2. åœ¨ SparkSession é…ç½®ä¸­æ·»åŠ  GCS æ–‡ä»¶ç³»ç»Ÿå®ç°
3. ä½¿ç”¨ `path.getFileSystem(hadoop_conf)` è€Œä¸æ˜¯ `FileSystem.get(hadoop_conf)`

---

### é—®é¢˜ 4ï¼šPort already allocated (8080)
**åŸå› **ï¼šç«¯å£ 8080 è¢«å ç”¨

**è§£å†³**ï¼š
```yaml
# docker-compose.yaml ä¿®æ”¹ç«¯å£æ˜ å°„
ports:
  - 18080:8080  # ä½¿ç”¨å…¶ä»–ç«¯å£
```

---

### é—®é¢˜ 5ï¼šPipeline å¡åœ¨æŸä¸ª Stage
**æ’æŸ¥**ï¼š
```bash
# æŸ¥çœ‹ Spark Master UI
open http://localhost:18080

# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker logs supply-chain-jupyterlab
docker logs supply-chain-spark-master
docker logs supply-chain-spark-worker-1

# æŸ¥çœ‹ Spark ä»»åŠ¡è¯¦æƒ…
docker exec supply-chain-spark-master cat /opt/spark-logs/spark-master.out
```

**å¸¸è§åŸå› **ï¼š
- Worker èµ„æºä¸è¶³
- ç½‘ç»œ I/O ç“¶é¢ˆï¼ˆGCS è¯»å†™ï¼‰
- åˆ†åŒºæ•°é‡ä¸åˆç†

---

## é™„å½•ï¼šå®Œæ•´æ‰§è¡Œè„šæœ¬

åˆ›å»ºä¸€é”®æ‰§è¡Œè„šæœ¬ `run_batch_pipeline.sh`ï¼š

```bash
#!/bin/bash
set -e

PROJECT_ROOT="/Users/liceline/Documents/study_material/data_engineer/project_study/supply_chain/supply_chain_de_study"
DOCKER_SPARK_DIR="$PROJECT_ROOT/docker/spark"

echo "ğŸš€ Starting Batch Pipeline Execution..."

# 1. åˆ›å»º Docker ç½‘ç»œï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
echo "ğŸ“¡ Creating Docker network..."
docker network create supply-chain-network 2>/dev/null || echo "Network already exists"

# 2. æ„å»º Docker é•œåƒ
echo "ğŸ”¨ Building Docker images..."
cd "$DOCKER_SPARK_DIR"
docker compose -f docker-compose.yaml --env-file .env build

# 3. å¯åŠ¨å®¹å™¨
echo "ğŸ³ Starting Docker containers..."
docker compose -f docker-compose.yaml --env-file .env up -d

# 4. ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ Waiting for services to start..."
sleep 10

# 5. éªŒè¯æœåŠ¡çŠ¶æ€
echo "âœ… Checking service status..."
docker ps --filter "name=supply-chain"

# 6. è¿è¡Œ Batch Pipeline
echo "ğŸ¯ Running Batch Pipeline..."
docker exec supply-chain-jupyterlab \
  python /opt/workspace/batch_pipeline/data_modeling/transformed_data.py

echo "ğŸ‰ Batch Pipeline execution completed!"
```

**ä½¿ç”¨æ–¹æ³•**ï¼š
```bash
chmod +x run_batch_pipeline.sh
./run_batch_pipeline.sh
```

---

## æ€»ç»“

### å…³é”®é…ç½®è¦ç‚¹
1. **Java 17**ï¼šSpark 3.5.0 çš„å¿…è¦æ¡ä»¶
2. **GCS è¿æ¥å™¨**ï¼šåŒæ—¶å¤åˆ¶åˆ° Spark å’Œ PySpark çš„ jars ç›®å½•
3. **GCS æ–‡ä»¶ç³»ç»Ÿé…ç½®**ï¼šåœ¨ SparkSession åˆ›å»ºå‰é…ç½®
4. **åˆ†åŒºç­–ç•¥**ï¼šé¿å… `coalesce(1)`ï¼Œä½¿ç”¨åˆç†çš„ `repartition`
5. **èµ„æºé…ç½®**ï¼šæ ¹æ®æ•°æ®é‡è°ƒæ•´ Worker æ•°é‡å’Œèµ„æº

### æ€§èƒ½æå‡è·¯å¾„
- **çŸ­æœŸ**ï¼ˆæœ¬åœ°å¼€å‘ï¼‰ï¼šå¢åŠ  Workerã€ä¼˜åŒ–é…ç½® â†’ 15-25 åˆ†é’Ÿ
- **é•¿æœŸ**ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰ï¼šè¿ç§»åˆ° Cloud Dataproc â†’ 10-15 åˆ†é’Ÿ

### ç›‘æ§ä¸è°ƒè¯•
- Spark Master UI: http://localhost:18080
- JupyterLab: http://localhost:8888
- æ—¥å¿—ï¼š`docker logs <container_name>`

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**æœ€åæ›´æ–°**ï¼š2026-02-05  
**ä½œè€…**ï¼šSupply Chain Data Platform Team
