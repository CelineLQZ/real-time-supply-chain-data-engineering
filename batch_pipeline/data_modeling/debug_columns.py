"""
è°ƒè¯•è„šæœ¬ï¼šæŸ¥çœ‹ GCS åŸå§‹æ•°æ®çš„å®é™…åˆ—å
"""
import pyspark
from pyspark.sql import SparkSession
from config import credentials_path, jar_file_path, input_path, GCP_PROJECT_ID

# Create Spark Session
spark = SparkSession.builder \
    .appName('debug-columns') \
    .config("spark.jars", jar_file_path) \
    .config("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS") \
    .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
    .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
    .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", credentials_path) \
    .config("spark.hadoop.google.cloud.project.id", GCP_PROJECT_ID) \
    .getOrCreate()

# Load data from GCS
print(f'Loading data from {input_path}')
df = spark.read.parquet(input_path + '*')

print(f'\nğŸ“Š åŸå§‹æ•°æ®ç»Ÿè®¡ï¼š')
print(f'   â†’ è¡Œæ•°ï¼š{df.count()}')
print(f'   â†’ åˆ—æ•°ï¼š{len(df.columns)}')

print(f'\nğŸ“‹ æ‰€æœ‰åˆ—åï¼ˆ{len(df.columns)} åˆ—ï¼‰ï¼š')
for i, col in enumerate(df.columns, 1):
    print(f'   {i:2d}. "{col}"')

# æŸ¥æ‰¾åŒ…å« "date" æˆ– "Date" çš„åˆ—
date_cols = [col for col in df.columns if 'date' in col.lower()]
if date_cols:
    print(f'\nğŸ“… åŒ…å« "date" çš„åˆ—ï¼š')
    for col in date_cols:
        print(f'   - "{col}"')

# æŸ¥æ‰¾åŒ…å« "order" çš„åˆ—
order_cols = [col for col in df.columns if 'order' in col.lower()]
if order_cols:
    print(f'\nğŸ¯ åŒ…å« "order" çš„åˆ—ï¼š')
    for col in order_cols:
        print(f'   - "{col}"')

spark.stop()
