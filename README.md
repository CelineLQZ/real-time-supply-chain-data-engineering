# Real-Time Supply Chain Data Engineering Platform

<div align="center">

![GitHub](https://img.shields.io/badge/GitHub-Actions-2088FF?logo=github)
![dbt](https://img.shields.io/badge/dbt-1.8.2-FF6849)
![BigQuery](https://img.shields.io/badge/Google-BigQuery-3DDC84?logo=google-cloud)
![Kafka](https://img.shields.io/badge/Apache-Kafka-7.4.0-231F20?logo=apache-kafka)
![Python](https://img.shields.io/badge/Python-3.11-3776AB?logo=python)
![License](https://img.shields.io/badge/License-MIT-green)

**A comprehensive data engineering platform combining real-time streaming and batch processing for supply chain analytics**

[Features](#features) â€¢ [Architecture](#architecture) â€¢ [Quick Start](#quick-start) â€¢ [Documentation](#documentation) â€¢ [Contributing](#contributing)

</div>

---

## ğŸ“‹ Overview

This project implements a **production-grade data engineering platform** for supply chain analytics, combining:
- ğŸ”´ **Real-time streaming** via Kafka
- âš™ï¸ **Batch processing** via Spark
- ğŸ“Š **Data modeling** via dbt
- â˜ï¸ **Cloud infrastructure** via Google Cloud Platform (GCS + BigQuery)
- ğŸ”„ **CI/CD automation** via GitHub Actions

The platform processes supply chain data (orders, inventory, shipping, customers) and produces analytical tables for business intelligence and decision-making.

---

## ğŸ¯ Features

### âœ¨ Core Capabilities

| Feature | Description |
|---------|-----------|
| **Real-time Streaming** | Kafka producer/consumer pipeline with configurable buffer sizes and fetch limits |
| **Batch ETL Pipeline** | Spark-based data transformation with automatic column standardization |
| **Data Warehouse** | BigQuery with dbt-managed dimensional modeling (staging + marts) |
| **Infrastructure as Code** | Terraform for GCS buckets, BigQuery datasets, and service accounts |
| **Automated Testing** | Python unit tests, dbt model tests, and integration tests |
| **CI/CD Workflows** | GitHub Actions for code quality, validation, and multi-environment deployment |
| **Docker Containerization** | Pre-configured images for Kafka, Spark, and Airflow |
| **Data Quality** | Duplicate detection, null value analysis, and data profiling |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Data       â”‚
â”‚  (CSV Files)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INGESTION LAYER                     â”‚
â”‚  â”œâ”€ Kafka Producer (Streaming)       â”‚
â”‚  â””â”€ Schema Registry & Control Center â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STORAGE LAYER (Bronze)              â”‚
â”‚  â”œâ”€ GCS: raw_streaming/              â”‚
â”‚  â”œâ”€ PostgreSQL (Local)               â”‚
â”‚  â””â”€ Kafka Topic: supply_chain_data   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRANSFORMATION LAYER                â”‚
â”‚  â”œâ”€ Spark Batch Pipeline             â”‚
â”‚  â”‚  â”œâ”€ Data Cleaning                 â”‚
â”‚  â”‚  â”œâ”€ Column Standardization        â”‚
â”‚  â”‚  â”œâ”€ Duplicate Removal             â”‚
â”‚  â”‚  â””â”€ Star Schema Generation        â”‚
â”‚  â””â”€ Processing Engine: PySpark 4.1.1 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STORAGE LAYER (Silver)              â”‚
â”‚  â”œâ”€ GCS: transformed_data/           â”‚
â”‚  â””â”€ Format: Parquet (Columnar)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOADING LAYER                       â”‚
â”‚  â”œâ”€ BigQuery Loader                  â”‚
â”‚  â”œâ”€ Project: stellar-stream-485314   â”‚
â”‚  â””â”€ Dataset: supply_chain_bigquery   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODELING LAYER (dbt)                â”‚
â”‚  â”œâ”€ Staging Models (6 views)         â”‚
â”‚  â”‚  â”œâ”€ dim_order.sql                 â”‚
â”‚  â”‚  â”œâ”€ dim_customer.sql              â”‚
â”‚  â”‚  â”œâ”€ dim_location.sql              â”‚
â”‚  â”‚  â”œâ”€ dim_product.sql               â”‚
â”‚  â”‚  â”œâ”€ dim_department.sql            â”‚
â”‚  â”‚  â””â”€ dim_shipping.sql              â”‚
â”‚  â””â”€ Mart Models (6 tables)           â”‚
â”‚     â”œâ”€ overall_performance.sql       â”‚
â”‚     â”œâ”€ inventory_levels.sql          â”‚
â”‚     â”œâ”€ customer_retention_rate.sql   â”‚
â”‚     â”œâ”€ financial_commitments.sql     â”‚
â”‚     â”œâ”€ fraud_detection.sql           â”‚
â”‚     â””â”€ payment_delays.sql            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ANALYTICS LAYER                     â”‚
â”‚  â”œâ”€ BI Tools (Looker, Tableau, etc) â”‚
â”‚  â””â”€ Data Scientists & Analysts       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Summary
1. **Source** â†’ CSV files and real-time events
2. **Ingestion** â†’ Kafka streaming pipeline
3. **Bronze** â†’ Raw data in GCS and Kafka topics
4. **Transformation** â†’ Spark batch processing (clean, standardize, deduplicate)
5. **Silver** â†’ Transformed parquet files in GCS
6. **Loading** â†’ BigQuery tables (facts + dimensions)
7. **Modeling** â†’ dbt staging views + analytical marts
8. **Analytics** â†’ BI dashboards and SQL queries

---

## ğŸš€ Quick Start

### Prerequisites

- **Python**: 3.10+
- **Java**: OpenJDK 11+ (required for Spark)
- **Docker & Docker Compose**: For containerized services
- **GCP Account**: Project with enabled services (BigQuery, Cloud Storage)
- **Git**: For version control

### Installation

#### 1. Clone the Repository
```bash
git clone https://github.com/CelineLQZ/real-time-supply-chain-data-engineering.git
cd supply_chain_de_study
```

#### 2. Set Up Python Environment
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r batch_pipeline/data_modeling/requirements.txt
pip install -r streaming_pipeline/kafka/requirements.txt
```

#### 3. Configure GCP Credentials
```bash
# Place GCP service account JSON in keys/ directory
cp /path/to/gcp-service-account.json keys/gcp-cred.json

# Set environment variable
export GOOGLE_APPLICATION_CREDENTIALS="$(pwd)/keys/gcp-cred.json"
```

#### 4. Set Up Docker Services
```bash
# Start Kafka, Zookeeper, Schema Registry, and Control Center
cd docker/kafka
docker-compose up -d

# Verify services are running
docker-compose ps
```

#### 5. Initialize dbt
```bash
cd dbt
dbt deps  # Install dbt packages
dbt parse  # Validate project structure
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ README.md                          # Project documentation
â”œâ”€â”€ CI_CD_STRATEGY.md                  # Detailed CI/CD documentation
â”œâ”€â”€ CI_CD_QUICKSTART.md                # Quick CI/CD setup guide
â”œâ”€â”€ pytest.ini                         # pytest configuration
â”œâ”€â”€ .pre-commit-config.yaml            # Pre-commit hooks
â”‚
â”œâ”€â”€ batch_pipeline/                    # Batch ETL Processing
â”‚   â”œâ”€â”€ run_batch_pipeline.sh          # Main pipeline script
â”‚   â””â”€â”€ data_modeling/
â”‚       â”œâ”€â”€ config.py                  # Batch pipeline config
â”‚       â”œâ”€â”€ requirements.txt           # Python dependencies
â”‚       â”œâ”€â”€ transformed_data.py        # Spark transformation logic
â”‚       â”œâ”€â”€ utilis.py                  # Helper functions & column mappings
â”‚       â””â”€â”€ __pycache__/               # Compiled Python files
â”‚
â”œâ”€â”€ streaming_pipeline/                # Real-time Streaming
â”‚   â””â”€â”€ kafka/
â”‚       â”œâ”€â”€ producer.py                # Kafka producer (data source)
â”‚       â”œâ”€â”€ consumer.py                # Kafka consumer (data sink)
â”‚       â”œâ”€â”€ kafka_config.py            # Kafka connection settings
â”‚       â”œâ”€â”€ requirements.txt           # Python dependencies
â”‚       â”œâ”€â”€ README.md                  # Kafka pipeline guide
â”‚       â””â”€â”€ __pycache__/               # Compiled files
â”‚
â”œâ”€â”€ dbt/                               # Data Transformation (DBT)
â”‚   â”œâ”€â”€ dbt_project.yml                # dbt project config
â”‚   â”œâ”€â”€ profiles.yml                   # dbt connection profiles
â”‚   â”œâ”€â”€ dbt_pipeline.md                # dbt pipeline documentation
â”‚   â”œâ”€â”€ dbt_cloud.yml                  # dbt cloud config
â”‚   â”œâ”€â”€ SOURCE_AND_DEDUPLICATION_GUIDE.md
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/                   # Staging models (6 dimensions)
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_order.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_customer.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_location.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_product.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_department.sql
â”‚   â”‚   â”‚   â””â”€â”€ dim_shipping.sql
â”‚   â”‚   â””â”€â”€ marts/                     # Mart models (6 analytics tables)
â”‚   â”‚       â”œâ”€â”€ overall_performance.sql
â”‚   â”‚       â”œâ”€â”€ inventory_levels.sql
â”‚   â”‚       â”œâ”€â”€ customer_retention_rate.sql
â”‚   â”‚       â”œâ”€â”€ financial_commitments.sql
â”‚   â”‚       â”œâ”€â”€ fraud_detection.sql
â”‚   â”‚       â””â”€â”€ payment_delays.sql
â”‚   â”œâ”€â”€ tests/                         # dbt tests (schema + data)
â”‚   â””â”€â”€ dbt-env/                       # dbt virtual environment
â”‚
â”œâ”€â”€ terraform/                         # Infrastructure as Code
â”‚   â”œâ”€â”€ main.tf                        # GCP resources (GCS, BigQuery)
â”‚   â”œâ”€â”€ variables.tf                   # Variable definitions
â”‚   â”œâ”€â”€ terraform.tfstate              # State file
â”‚   â”œâ”€â”€ tfplan                         # Execution plan
â”‚   â””â”€â”€ terraform_note.md              # Terraform documentation
â”‚
â”œâ”€â”€ docker/                            # Docker Configurations
â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â””â”€â”€ docker-compose.yaml        # Kafka stack
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”œâ”€â”€ docker-compose.yaml        # Spark cluster
â”‚   â”‚   â”œâ”€â”€ spark-base.Dockerfile
â”‚   â”‚   â”œâ”€â”€ spark-master.Dockerfile
â”‚   â”‚   â”œâ”€â”€ spark-worker.Dockerfile
â”‚   â”‚   â”œâ”€â”€ cluster-base.Dockerfile
â”‚   â”‚   â”œâ”€â”€ jupyterlab.Dockerfile
â”‚   â”‚   â””â”€â”€ jar_files/                 # Spark dependencies
â”‚   â””â”€â”€ airflow/
â”‚       â”œâ”€â”€ docker-compose.yaml        # Airflow orchestration
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â”œâ”€â”€ config/
â”‚       â”‚   â””â”€â”€ airflow.cfg
â”‚       â”œâ”€â”€ dags/                      # Airflow DAGs
â”‚       â”œâ”€â”€ logs/                      # Airflow execution logs
â”‚       â””â”€â”€ plugins/                   # Custom Airflow operators
â”‚
â”œâ”€â”€ data/                              # Sample Data
â”‚   â”œâ”€â”€ DataCoSupplyChainDataset.csv   # Main supply chain dataset
â”‚   â”œâ”€â”€ DescriptionDataCoSupplyChain.csv
â”‚   â”œâ”€â”€ tokenized_access_logs.csv
â”‚   â”œâ”€â”€ data_exploration.ipynb         # Data profiling notebook
â”‚   â”œâ”€â”€ data_view.py                   # CLI data exploration tool
â”‚   â””â”€â”€ data_dictionary.csv            # Generated metadata
â”‚
â”œâ”€â”€ configs/                           # Global Configuration
â”‚   â””â”€â”€ config.py                      # Shared settings
â”‚
â”œâ”€â”€ keys/                              # Credentials (âš ï¸ .gitignored)
â”‚   â””â”€â”€ gcp-cred.json                  # GCP service account (DO NOT COMMIT)
â”‚
â”œâ”€â”€ logs/                              # Execution Logs
â”‚   â””â”€â”€ batch_pipeline/                # Batch job logs
â”‚
â”œâ”€â”€ note/                              # Documentation
â”‚   â”œâ”€â”€ DATA_PIPELINE_FLOW.md          # Data flow visualization
â”‚   â”œâ”€â”€ BATCH_DATA_PROCESSING.md       # Batch processing guide
â”‚   â”œâ”€â”€ STREAMING_PIPELINE_GUIDE.md    # Streaming setup guide
â”‚   â”œâ”€â”€ INGESTION_TRANSFORM_UPLOAD_GUIDE.md
â”‚   â”œâ”€â”€ PHASE1_INGESTION_DETAILED_GUIDE.md
â”‚   â””â”€â”€ TERRAFORM_GUIDE.md             # Infrastructure guide
â”‚
â”œâ”€â”€ tests/                             # Python Unit Tests
â”‚   â”œâ”€â”€ test_batch_pipeline.py
â”‚   â”œâ”€â”€ test_kafka_pipeline.py
â”‚   â””â”€â”€ test_data_quality.py
â”‚
â”œâ”€â”€ .github/                           # GitHub Configuration
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                     # CI pipeline (linting, tests, validation)
â”‚       â””â”€â”€ deploy.yml                 # CD pipeline (dev/staging/prod deployment)
â”‚
â””â”€â”€ .gitignore                         # Git ignore rules
```

---

## ğŸ”§ Configuration

### Batch Pipeline Configuration
**File**: `batch_pipeline/data_modeling/config.py`

```python
# Spark Configuration
SPARK_APP_NAME = "supply_chain_etl"
SPARK_MASTER = "local[*]"  # or "spark://master:7077" for cluster

# GCP Configuration
GCP_PROJECT_ID = "stellar-stream-485314"
GCS_BUCKET = "supply-chain-data-bucket-485314"
BIGQUERY_DATASET = "supply_chain_bigquery"

# Data Paths
GCS_RAW_PATH = f"gs://{GCS_BUCKET}/raw_streaming"
GCS_TRANSFORM_PATH = f"gs://{GCS_BUCKET}/transformed_data"
```

### Streaming Pipeline Configuration
**File**: `streaming_pipeline/kafka/kafka_config.py`

```python
KAFKA_BOOTSTRAP_SERVERS = ["localhost:9092"]
KAFKA_TOPIC = "supply_chain_data"
KAFKA_GROUP_ID = "supply_chain_consumer_group"

# Producer Settings
PRODUCER_BATCH_SIZE = 16384
PRODUCER_LINGER_MS = 100

# Consumer Settings
CONSUMER_FETCH_MIN_BYTES = 1024
CONSUMER_FETCH_MAX_WAIT_MS = 500
CONSUMER_BUFFER_SIZE = 20000
CONSUMER_FETCH_MAX_BYTES = 52428800
```

### dbt Configuration
**File**: `dbt/profiles.yml`

```yaml
supply_chain:
  target: dev
  outputs:
    dev:
      type: bigquery
      project: stellar-stream-485314
      dataset: supply_chain_bigquery_dev
      method: service-account
      keyfile: ../keys/gcp-cred.json
      
    staging:
      type: bigquery
      project: stellar-stream-485314
      dataset: supply_chain_bigquery_staging
      
    prod:
      type: bigquery
      project: stellar-stream-485314
      dataset: supply_chain_bigquery_prod
```

---

## ğŸ“Š Running the Pipeline

### 1. Streaming Pipeline (Kafka)

#### Start Kafka Services
```bash
cd docker/kafka
docker-compose up -d
```

#### Run Producer (Data Source)
```bash
cd streaming_pipeline/kafka
python producer.py
```

#### Run Consumer (Data Sink)
```bash
cd streaming_pipeline/kafka
python consumer.py
```

### 2. Batch Pipeline (Spark)

#### Execute Full Pipeline
```bash
cd batch_pipeline
bash run_batch_pipeline.sh
```

#### Or Run Components Separately
```bash
# Configure and run batch ETL
cd batch_pipeline/data_modeling
python transformed_data.py

# Load data to BigQuery
python bigquery_loader.py
```

### 3. dbt Modeling

#### Run All Models
```bash
cd dbt
dbt run
```

#### Run Specific Models
```bash
# Staging models only
dbt run --select staging

# Specific mart
dbt run --select overall_performance

# Models + tests
dbt build
```

#### Test Data Quality
```bash
dbt test
```

---

## ğŸ§ª Testing

### Python Unit Tests
```bash
# Run all tests
pytest tests/

# Run specific test file
pytest tests/test_batch_pipeline.py -v

# Run with coverage
pytest --cov=batch_pipeline --cov=streaming_pipeline tests/
```

### dbt Tests
```bash
cd dbt

# Run schema tests
dbt test

# Run specific test
dbt test --select dim_order
```

### Integration Tests
```bash
# Test end-to-end pipeline
bash scripts/run_integration_tests.sh
```

---

## ğŸ”„ CI/CD Pipeline

This project uses **GitHub Actions** for automated validation and deployment.

### CI Workflow (`.github/workflows/ci.yml`)

Runs on every push and pull request:

1. **Code Quality** (Parallel)
   - Pylint: Python linting
   - Black: Code formatting
   - flake8: Style guide enforcement

2. **Unit Tests** (Parallel)
   - Python unit tests (pytest)
   - dbt model tests

3. **Infrastructure Validation** (Parallel)
   - Terraform plan (detects resource changes)
   - dbt parser (validates models)

4. **Build Artifacts**
   - Docker image build and push (optional)

**Total CI Time**: ~5 minutes

### CD Workflow (`.github/workflows/deploy.yml`)

Triggered manually for deployment:

1. **Pre-deployment Checks**
   - Branch verification
   - Credential validation

2. **Infrastructure Setup** â†’ dbt Models â†’ Docker Images â†’ Integration Tests

3. **Environment-specific Deployment**
   - `dev`: Immediate deployment
   - `staging`: Code review approval required
   - `prod`: Manual approval + Slack notification

---

## ğŸ“ˆ Data Models

### Staging Models (Dimensional Views)

| Model | Description | Rows | Columns |
|-------|-------------|------|---------|
| `dim_order` | Order details with items and profit | 631,301 | 18 |
| `dim_customer` | Customer demographics and location | ~2,500 | 10 |
| `dim_location` | Geographic information | ~1,000 | 8 |
| `dim_product` | Product catalog and categories | ~500 | 6 |
| `dim_department` | Department and sales info | ~5 | 4 |
| `dim_shipping` | Shipping and delivery metrics | 631,301 | 12 |

### Mart Models (Analytical Tables)

| Mart | Purpose | Update Frequency | Users |
|------|---------|------------------|-------|
| `overall_performance` | Monthly KPIs (sales, profit, shipping) | Daily | Executives |
| `inventory_levels` | Inventory aging and turnover analysis | Weekly | Supply Chain |
| `customer_retention_rate` | Monthly customer retention metrics | Daily | Marketing |
| `financial_commitments` | Department-product financial analysis | Weekly | Finance |
| `fraud_detection` | High-value customer anomalies | Real-time | Risk |
| `payment_delays` | Shipping delays and delivery status | Daily | Operations |

---

## ğŸ“Š Data Exploration

### Jupyter Notebook
```bash
# Open data exploration notebook
jupyter notebook data/data_exploration.ipynb
```

The notebook includes:
- Data loading and encoding detection
- Duplicate detection analysis
- Data type profiling
- Numeric range analysis
- Categorical value analysis
- Missing value investigation
- Data quality summary

### CLI Tool
```bash
# Interactive data exploration
python data/data_view.py
```

---

## ğŸ“š Documentation

| Document | Purpose |
|----------|---------|
| [CI_CD_STRATEGY.md](CI_CD_STRATEGY.md) | Comprehensive CI/CD architecture and implementation |
| [CI_CD_QUICKSTART.md](CI_CD_QUICKSTART.md) | Quick setup guide for CI/CD pipelines |
| [note/DATA_PIPELINE_FLOW.md](note/DATA_PIPELINE_FLOW.md) | Data flow diagrams and architecture |
| [note/BATCH_DATA_PROCESSING.md](note/BATCH_DATA_PROCESSING.md) | Batch ETL pipeline details |
| [note/STREAMING_PIPELINE_GUIDE.md](note/STREAMING_PIPELINE_GUIDE.md) | Kafka streaming setup |
| [note/TERRAFORM_GUIDE.md](note/TERRAFORM_GUIDE.md) | Infrastructure provisioning |
| [dbt/dbt_pipeline.md](dbt/dbt_pipeline.md) | dbt model architecture |
| [dbt/SOURCE_AND_DEDUPLICATION_GUIDE.md](dbt/SOURCE_AND_DEDUPLICATION_GUIDE.md) | Data deduplication strategy |

---

## ğŸ”‘ Key Technologies

| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Streaming** | Apache Kafka | 7.4.0 | Real-time data ingestion |
| **Processing** | Apache Spark | 4.1.1 | Batch data transformation |
| **Data Warehouse** | Google BigQuery | Latest | Columnar data storage |
| **Cloud Storage** | Google Cloud Storage | Latest | Data lake (Bronze/Silver) |
| **Data Modeling** | dbt | 1.8.2 | SQL-based transformations |
| **Python** | Python | 3.11.14 | Data processing logic |
| **IaC** | Terraform | Latest | Infrastructure provisioning |
| **Containers** | Docker & Docker Compose | Latest | Service containerization |
| **Orchestration** | Airflow | Latest | Workflow scheduling (optional) |
| **CI/CD** | GitHub Actions | Latest | Automated testing & deployment |

---

## ğŸ› ï¸ Troubleshooting

### Kafka Connection Issues
```bash
# Check if Kafka is running
docker ps | grep kafka

# View Kafka logs
docker logs <kafka-container-id>

# Test connection
nc -zv localhost 9092
```

### Spark Memory Errors
```bash
# Increase Spark memory allocation
export SPARK_DRIVER_MEMORY=4g
export SPARK_EXECUTOR_MEMORY=4g
```

### BigQuery Credentials
```bash
# Verify GCP credentials
gcloud auth application-default print-access-token

# Set credentials path
export GOOGLE_APPLICATION_CREDENTIALS="$(pwd)/keys/gcp-cred.json"
```

### dbt Model Failures
```bash
# Validate dbt project structure
dbt parse

# Debug model execution
dbt run --debug

# Check model dependencies
dbt dag
```

---

## ğŸ“ Environment Variables

Create a `.env` file in the project root:

```bash
# GCP Configuration
GCP_PROJECT_ID=stellar-stream-485314
GCS_BUCKET=supply-chain-data-bucket-485314
GOOGLE_APPLICATION_CREDENTIALS=/path/to/gcp-cred.json

# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC=supply_chain_data

# Database Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=supply_chain
POSTGRES_USER=admin
POSTGRES_PASSWORD=password

# Spark Configuration
SPARK_MASTER=local[*]
SPARK_DRIVER_MEMORY=4g
SPARK_EXECUTOR_MEMORY=4g

# Environment
ENVIRONMENT=dev  # dev, staging, prod
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Make** your changes
4. **Commit** with clear messages (`git commit -m 'Add amazing feature'`)
5. **Push** to the branch (`git push origin feature/amazing-feature`)
6. **Open** a Pull Request

### Code Standards
- Python: Follow [PEP 8](https://www.python.org/dev/peps/pep-0008/) (enforced by Black)
- SQL: Use lower_case_with_underscores for identifiers
- dbt: Follow dbt [style guide](https://docs.getdbt.com/guides/best-practices/how-we-style/styling-guide-overview)
- Commits: Use [conventional commits](https://www.conventionalcommits.org/)

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE) - see the LICENSE file for details.

---

## ğŸ‘¥ Authors

- **Celine Li** - Data Engineer
- *Project started*: February 2026
- *Last updated*: February 2026

---

## ğŸ™ Acknowledgments

- Apache Kafka & Spark communities
- Google Cloud Platform documentation
- dbt Labs and community
- Contributors and reviewers

---

## ğŸ“ Support

For issues, questions, or suggestions:

1. **GitHub Issues**: [Create an issue](https://github.com/CelineLQZ/real-time-supply-chain-data-engineering/issues)
2. **Documentation**: Check [docs](note/) directory
3. **Email**: Submit questions with reproducible examples

---

<div align="center">

**[â¬† back to top](#real-time-supply-chain-data-engineering-platform)**

Made with â¤ï¸ by Celine Li | Star â­ if you find this project helpful!

</div>
